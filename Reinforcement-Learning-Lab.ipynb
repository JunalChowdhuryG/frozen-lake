{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Laboratorio de aprendizaje por refuerzo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo es un enfoque general para aprender una política de control en sistemas dinámicos estocásticos a través de la interacción. En este paradigma, un agente aprende a optimizar su comportamiento acumulando recompensas con el tiempo. A diferencia de la optimización local, el aprendizaje por refuerzo se enfoca en descubrir el control óptimo incluso cuando las recompensas están diferidas en el tiempo.\n",
    "\n",
    "En este laboratorio, exploraremos dos algoritmos clave:\n",
    "1. **SARSA** (*State-Action-Reward-State-Action*): Un algoritmo *on-policy* que actualiza los valores de las acciones en función de la política que sigue el agente.\n",
    "2. **Q-Learning**: Un algoritmo *off-policy* que estima el valor de la política óptima independientemente de la política actual del agente.\n",
    "\n",
    "Además, abordaremos el dilema de **exploración-explotación**, donde el agente debe balancear entre:\n",
    "- **Explorar**: Probar nuevas acciones para descubrir más sobre el entorno.\n",
    "- **Explotar**: Utilizar lo aprendido para maximizar las recompensas conocidas.\n",
    "\n",
    "Para simplificar el aprendizaje de los algoritmos, utilizaremos el entorno **FrozenLake-v0** de OpenAI Gym, un problema sencillo que permite ilustrar los conceptos básicos del aprendizaje por refuerzo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marco Teórico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Elementos del Aprendizaje por Refuerzo**\n",
    "1. **Agente**: Entidad que toma decisiones.\n",
    "2. **Entorno**: Sistema en el que opera el agente.\n",
    "3. **Estados ($ S $)**: Configuraciones posibles del entorno.\n",
    "4. **Acciones ($ A $)**: Conjunto de decisiones posibles para el agente.\n",
    "5. **Recompensas ($ R $)**: Retroalimentación numérica que guía al agente.\n",
    "6. **Política ($ \\pi $)**: Estrategia que sigue el agente para tomar decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Elementos del Aprendizaje por Refuerzo en el Contexto de FrozenLake**\n",
    "\n",
    "#### 1. **Agente**:\n",
    "- **Definición**: El agente es quien toma decisiones en el entorno para maximizar la recompensa acumulada.\n",
    "- **En FrozenLake**: \n",
    "  - El agente es el jugador que debe moverse desde la posición inicial `S` (Start) hasta la meta `G` (Goal) evitando caer en los agujeros `H`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Entorno**:\n",
    "- **Definición**: El sistema en el que opera el agente. Proporciona estados y recompensas en respuesta a las acciones tomadas por el agente.\n",
    "- **En FrozenLake**:\n",
    "  - El entorno es el mapa representado como una cuadrícula:\n",
    "    - `S`: Inicio.\n",
    "    - `F`: Hielo congelado (camino seguro).\n",
    "    - `H`: Agujeros (estado terminal negativo).\n",
    "    - `G`: Meta (estado terminal positivo).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Estados ($ S $)**:\n",
    "- **Definición**: Representan las configuraciones posibles del entorno en un momento dado.\n",
    "- **En FrozenLake**:\n",
    "  - Los estados corresponden a las posiciones del agente en la cuadrícula.\n",
    "  - Por ejemplo, en un mapa de 4x4, hay $ 16 $ estados posibles numerados de $ 0 $ a $ 15 $.\n",
    "  - El estado se calcula como:\n",
    "    \\[\n",
    "    \\text{Estado} = \\text{fila} \\times \\text{número de columnas} + \\text{columna}\n",
    "    \\]\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Acciones ($ A $)**:\n",
    "- **Definición**: Conjunto de decisiones que el agente puede tomar desde un estado dado.\n",
    "- **En FrozenLake**:\n",
    "  - El agente tiene 4 acciones disponibles:\n",
    "    - `0`: Izquierda.\n",
    "    - `1`: Abajo.\n",
    "    - `2`: Derecha.\n",
    "    - `3`: Arriba.\n",
    "  - En modo resbaladizo (`is_slippery=True`), la acción tomada puede no coincidir con el movimiento efectivo debido a la estocasticidad.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Recompensas ($ R $)**:\n",
    "- **Definición**: Retroalimentación numérica que guía al agente hacia su objetivo.\n",
    "- **En FrozenLake**:\n",
    "  - Las recompensas están definidas como:\n",
    "    - **Meta (`G`)**: $ +1 $.\n",
    "    - **Hielo (`F`) o Inicio (`S`)**: $ 0 $.\n",
    "    - **Agujero (`H`)**: $ 0 $.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Política ($ \\pi $)**:\n",
    "- **Definición**: Estrategia que sigue el agente para tomar decisiones en cada estado.\n",
    "- **En FrozenLake**:\n",
    "  - La política puede ser:\n",
    "    - **Inicial**: Selección aleatoria de acciones (exploración).\n",
    "    - **Aprendida**: Después de entrenar al agente, la política selecciona la acción con el mayor valor $ Q(s, a) $ en cada estado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Algoritmos de Aprendizaje**\n",
    "\n",
    "1. **SARSA**:\n",
    "   - SARSA es un algoritmo *on-policy*, lo que significa que actualiza los valores $ Q(s, a) $ basándose en la política actual.\n",
    "   - Ecuación de actualización:\n",
    "     $$\n",
    "     Q_{t+1}(s_t, a_t) \\leftarrow Q_t(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q_t(s_{t+1}, a_{t+1}) - Q_t(s_t, a_t) \\right]\n",
    "     $$\n",
    "     Donde:\n",
    "     - $ \\alpha $: Tasa de aprendizaje.\n",
    "     - $ \\gamma $: Factor de descuento.\n",
    "     - $ r_t $: Recompensa en el paso $ t $.\n",
    "\n",
    "2. **Q-Learning**:\n",
    "   - Q-Learning es un algoritmo *off-policy* que busca la política óptima sin seguir necesariamente la política actual.\n",
    "   - Ecuación de actualización:\n",
    "     $$\n",
    "     Q_{t+1}(s_t, a_t) \\leftarrow Q_t(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_b Q_t(s_{t+1}, b) - Q_t(s_t, a_t) \\right]\n",
    "     $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Estrategias de Exploración**\n",
    "1. **ε-greedy**:\n",
    "   - Con probabilidad $ 1-\\epsilon $, el agente elige la mejor acción conocida.\n",
    "   - Con probabilidad $ \\epsilon $, selecciona una acción aleatoria para explorar.\n",
    "2. **Softmax**:\n",
    "   - Asigna probabilidades a cada acción según sus valores $ Q(s, a) $:\n",
    "     $$\n",
    "     P(a_i|s) = \\frac{e^{\\frac{Q(s, a_i)}{\\tau}}}{\\sum_j e^{\\frac{Q(s, a_j)}{\\tau}}}\n",
    "     $$\n",
    "     Donde $ \\tau $ controla el nivel de exploración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluación y Rendimiento**\n",
    "1. Para evaluar el aprendizaje, es esencial contar los éxitos en ventanas de episodios (por ejemplo, 100).\n",
    "2. Las políticas aprendidas deben evaluarse repetidamente para obtener un promedio confiable, especialmente en entornos estocásticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reglas del juego\n",
    "import random\n",
    "\n",
    "class FrozenLake:\n",
    "    def __init__(self, map_desc=None, is_slippery=True):\n",
    "        self.map_desc = map_desc or [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ]\n",
    "        self.is_slippery = is_slippery\n",
    "        self.n_rows = len(self.map_desc)\n",
    "        self.n_cols = len(self.map_desc[0])\n",
    "        self.start_state = self._find_start()\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "\n",
    "        # Diccionario de movimientos (acciones) corregido\n",
    "        self.actions = {\n",
    "            0: (0, -1),  # Izquierda\n",
    "            1: (1, 0),   # Abajo\n",
    "            2: (0, 1),   # Derecha\n",
    "            3: (-1, 0)   # Arriba\n",
    "        }\n",
    "\n",
    "    def _find_start(self):\n",
    "        \"\"\"Encuentra la posición inicial (S) en el mapa.\"\"\"\n",
    "        for row in range(self.n_rows):\n",
    "            for col in range(self.n_cols):\n",
    "                if self.map_desc[row][col] == \"S\":\n",
    "                    return (row, col)\n",
    "        raise ValueError(\"Mapa no contiene estado inicial 'S'.\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el juego al estado inicial.\"\"\"\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "        return self.state\n",
    "\n",
    "    def _inside_bounds(self, row, col):\n",
    "        \"\"\"Verifica si la posición está dentro del mapa.\"\"\"\n",
    "        return 0 <= row < self.n_rows and 0 <= col < self.n_cols\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Aplica una acción y devuelve el nuevo estado, recompensa, y si terminó.\"\"\"\n",
    "        if self.terminated:\n",
    "            raise ValueError(\"El episodio ya terminó. Reinicia el juego con reset().\")\n",
    "\n",
    "        # Probabilidad de resbalar\n",
    "        if self.is_slippery:\n",
    "            action = random.choice(list(self.actions.keys()))\n",
    "\n",
    "        # Determinar la nueva posición\n",
    "        move = self.actions[action]\n",
    "        new_row = self.state[0] + move[0]\n",
    "        new_col = self.state[1] + move[1]\n",
    "\n",
    "        # Si la nueva posición está fuera de los límites, el agente no se mueve\n",
    "        if not self._inside_bounds(new_row, new_col):\n",
    "            new_row, new_col = self.state\n",
    "\n",
    "        # Actualizar estado\n",
    "        new_state = (new_row, new_col)\n",
    "        tile = self.map_desc[new_row][new_col]\n",
    "\n",
    "        # Determinar recompensa y si terminó el episodio\n",
    "        reward = 0\n",
    "        if tile == \"H\":  # Agujero\n",
    "            self.terminated = True\n",
    "        elif tile == \"G\":  # Meta\n",
    "            reward = 1\n",
    "            self.terminated = True\n",
    "\n",
    "        self.state = new_state\n",
    "        return new_state, reward, self.terminated\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Muestra el mapa actual con la posición del agente.\"\"\"\n",
    "        for row in range(self.n_rows):\n",
    "            row_str = \"\"\n",
    "            for col in range(self.n_cols):\n",
    "                if (row, col) == self.state:\n",
    "                    row_str += \"A\"  # Agente\n",
    "                else:\n",
    "                    row_str += self.map_desc[row][col]\n",
    "            print(row_str)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicios**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 1: Familiarización con OpenAI Gym**\n",
    "Este ejercicio tiene como propósito entender cómo interactuar con el entorno FrozenLake-v1 y explorar sus características principales. Esto incluye:\n",
    "- **Objetivo**:\n",
    "  - Familiarizarse con los espacios de observación y acción.\n",
    "  - Observar cómo las acciones afectan el estado y la recompensa.\n",
    "  - Experimentar con los parámetros clave, como:\n",
    "    - Mapas personalizados.\n",
    "    - Naturaleza resbaladiza del entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import random\n",
    "\n",
    "class FrozenLakeVisualizer:\n",
    "    def __init__(self, root, lake):\n",
    "        self.root = root\n",
    "        self.lake = lake\n",
    "        self.tile_size = 100  # Tamaño de cada celda en píxeles\n",
    "        self.grid_size = (lake.n_rows, lake.n_cols)\n",
    "        self.state = lake.reset()\n",
    "        self.done = False\n",
    "\n",
    "        # Cargar las imágenes\n",
    "        self.load_images()\n",
    "\n",
    "        # Crear el canvas para el mapa\n",
    "        self.canvas = tk.Canvas(\n",
    "            root, width=self.grid_size[1] * self.tile_size, height=self.grid_size[0] * self.tile_size\n",
    "        )\n",
    "        self.canvas.pack()\n",
    "        self.draw_map()\n",
    "\n",
    "        # Botones\n",
    "        self.action_buttons = {\n",
    "            \"Izquierda\": 0,\n",
    "            \"Abajo\": 1,\n",
    "            \"Derecha\": 2,\n",
    "            \"Arriba\": 3\n",
    "        }\n",
    "        self.create_buttons()\n",
    "\n",
    "        # Mensajes\n",
    "        self.message_label = tk.Label(root, text=\"Presiona un botón para mover al agente.\")\n",
    "        self.message_label.pack()\n",
    "\n",
    "    def load_images(self):\n",
    "        \"\"\"Carga las imágenes necesarias desde la carpeta 'img' y las redimensiona.\"\"\"\n",
    "        self.images = {}\n",
    "        img_folder = \"img\"\n",
    "\n",
    "        try:\n",
    "            # Cargar imágenes de celdas\n",
    "            self.images[\"S\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"F\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"H\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"hole.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"G\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"goal.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "\n",
    "            # Cargar imágenes del agente\n",
    "            self.images[\"elf_down\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"elf_down.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"elf_left\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"elf_left.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"elf_right\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"elf_right.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"elf_up\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"elf_up.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error al cargar imágenes: {e}\")\n",
    "            print(\"Asegúrate de que la carpeta 'img' y las imágenes necesarias existan.\")\n",
    "            exit()\n",
    "\n",
    "    def create_buttons(self):\n",
    "        \"\"\"Crea los botones de acción.\"\"\"\n",
    "        self.button_frame = tk.Frame(self.root)\n",
    "        self.button_frame.pack()\n",
    "        for action, code in self.action_buttons.items():\n",
    "            button = tk.Button(self.button_frame, text=action, command=lambda c=code: self.take_step(c))\n",
    "            button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        # Botón para reiniciar\n",
    "        self.reset_button = tk.Button(self.root, text=\"Reiniciar\", command=self.reset_game)\n",
    "        self.reset_button.pack(pady=10)\n",
    "\n",
    "    def draw_map(self):\n",
    "        \"\"\"Dibuja el mapa del entorno con imágenes redimensionadas.\"\"\"\n",
    "        self.canvas.delete(\"all\")  # Limpiar el canvas\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                x, y = j * self.tile_size, i * self.tile_size\n",
    "                tile = self.lake.map_desc[i][j]\n",
    "                self.canvas.create_image(x, y, anchor=tk.NW, image=self.images[tile])\n",
    "\n",
    "        # Dibujar al agente\n",
    "        self.draw_agent()\n",
    "\n",
    "    def draw_agent(self):\n",
    "        \"\"\"Dibuja al agente en la posición actual con la imagen correspondiente.\"\"\"\n",
    "        x, y = self.state\n",
    "        x_pixel, y_pixel = y * self.tile_size, x * self.tile_size\n",
    "\n",
    "        # Seleccionar la imagen del agente según la acción\n",
    "        action_to_image = {\n",
    "            0: \"elf_left\",\n",
    "            1: \"elf_down\",\n",
    "            2: \"elf_right\",\n",
    "            3: \"elf_up\"\n",
    "        }\n",
    "        agent_image = action_to_image.get(getattr(self, \"last_action\", 1), \"elf_down\")\n",
    "        self.canvas.create_image(x_pixel, y_pixel, anchor=tk.NW, image=self.images[agent_image])\n",
    "\n",
    "    def take_step(self, action):\n",
    "        \"\"\"Realiza un paso en el juego.\"\"\"\n",
    "        if self.done:\n",
    "            self.message_label.config(text=\"El episodio ya terminó. Presiona 'Reiniciar' para empezar de nuevo.\")\n",
    "            return\n",
    "\n",
    "        # Guardar la última acción para la imagen del agente\n",
    "        self.last_action = action\n",
    "\n",
    "        # Realizar el paso\n",
    "        next_state, reward, terminated = self.lake.step(action)\n",
    "        self.state = next_state\n",
    "        self.done = terminated\n",
    "        self.draw_map()\n",
    "\n",
    "        # Actualizar mensajes\n",
    "        if self.done:\n",
    "            if reward > 0:\n",
    "                self.message_label.config(text=\"¡Ganaste! Llegaste a la meta.\")\n",
    "            else:\n",
    "                self.message_label.config(text=\"¡Perdiste! Caíste en un agujero.\")\n",
    "        else:\n",
    "            self.message_label.config(text=f\"Estado: {self.state}, Acción: {action}, Recompensa: {reward}\")\n",
    "\n",
    "    def reset_game(self):\n",
    "        \"\"\"Reinicia el juego.\"\"\"\n",
    "        self.state = self.lake.reset()\n",
    "        self.done = False\n",
    "        self.last_action = None  # No hay acción previa al reiniciar\n",
    "        self.draw_map()\n",
    "        self.message_label.config(text=\"Presiona un botón para mover al agente.\")\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=False)\n",
    "\n",
    "# Crear la ventana de Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"FrozenLake con Imágenes Redimensionadas\")\n",
    "app = FrozenLakeVisualizer(root, lake)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import random\n",
    "\n",
    "class FrozenLakeVisualizer:\n",
    "    def __init__(self, root, lake):\n",
    "        self.root = root\n",
    "        self.lake = lake\n",
    "        self.tile_size = 100  # Tamaño de cada celda en píxeles\n",
    "        self.grid_size = (lake.n_rows, lake.n_cols)\n",
    "        self.state = lake.reset()\n",
    "        self.done = False\n",
    "\n",
    "        # Cargar las imágenes\n",
    "        self.load_images()\n",
    "\n",
    "        # Crear el canvas para el mapa\n",
    "        self.canvas = tk.Canvas(\n",
    "            root, width=self.grid_size[1] * self.tile_size, height=self.grid_size[0] * self.tile_size\n",
    "        )\n",
    "        self.canvas.pack()\n",
    "        self.draw_map()\n",
    "\n",
    "        # Botones\n",
    "        self.action_buttons = {\n",
    "            \"Izquierda\": 0,\n",
    "            \"Abajo\": 1,\n",
    "            \"Derecha\": 2,\n",
    "            \"Arriba\": 3\n",
    "        }\n",
    "        self.create_buttons()\n",
    "\n",
    "        # Mensajes\n",
    "        self.message_label = tk.Label(root, text=\"Presiona un botón para mover al agente.\")\n",
    "        self.message_label.pack()\n",
    "\n",
    "    def load_images(self):\n",
    "        \"\"\"Carga las imágenes necesarias desde la carpeta 'img' y las redimensiona.\"\"\"\n",
    "        self.images = {}\n",
    "        img_folder = \"img\"\n",
    "\n",
    "        try:\n",
    "            # Cargar imágenes de celdas\n",
    "            self.images[\"S\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"F\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"H\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"hole.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"G\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"goal.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "\n",
    "            # Cargar imágenes del agente\n",
    "            self.images[\"elf_down\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"elf_down.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"elf_left\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"elf_left.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"elf_right\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"elf_right.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "            self.images[\"elf_up\"] = ImageTk.PhotoImage(\n",
    "                Image.open(os.path.join(img_folder, \"elf_up.png\")).resize((self.tile_size, self.tile_size))\n",
    "            )\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error al cargar imágenes: {e}\")\n",
    "            print(\"Asegúrate de que la carpeta 'img' y las imágenes necesarias existan.\")\n",
    "            exit()\n",
    "\n",
    "    def create_buttons(self):\n",
    "        \"\"\"Crea los botones de acción.\"\"\"\n",
    "        self.button_frame = tk.Frame(self.root)\n",
    "        self.button_frame.pack()\n",
    "        for action, code in self.action_buttons.items():\n",
    "            button = tk.Button(self.button_frame, text=action, command=lambda c=code: self.take_step(c))\n",
    "            button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        # Botón para reiniciar\n",
    "        self.reset_button = tk.Button(self.root, text=\"Reiniciar\", command=self.reset_game)\n",
    "        self.reset_button.pack(pady=10)\n",
    "\n",
    "    def draw_map(self):\n",
    "        \"\"\"Dibuja el mapa del entorno con imágenes redimensionadas.\"\"\"\n",
    "        self.canvas.delete(\"all\")  # Limpiar el canvas\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                x, y = j * self.tile_size, i * self.tile_size\n",
    "                tile = self.lake.map_desc[i][j]\n",
    "                self.canvas.create_image(x, y, anchor=tk.NW, image=self.images[tile])\n",
    "\n",
    "        # Dibujar al agente\n",
    "        self.draw_agent()\n",
    "\n",
    "    def draw_agent(self):\n",
    "        \"\"\"Dibuja al agente en la posición actual con la imagen correspondiente.\"\"\"\n",
    "        x, y = self.state\n",
    "        x_pixel, y_pixel = y * self.tile_size, x * self.tile_size\n",
    "\n",
    "        # Seleccionar la imagen del agente según la acción\n",
    "        action_to_image = {\n",
    "            0: \"elf_left\",\n",
    "            1: \"elf_down\",\n",
    "            2: \"elf_right\",\n",
    "            3: \"elf_up\"\n",
    "        }\n",
    "        agent_image = action_to_image.get(getattr(self, \"last_action\", 1), \"elf_down\")\n",
    "        self.canvas.create_image(x_pixel, y_pixel, anchor=tk.NW, image=self.images[agent_image])\n",
    "\n",
    "    def take_step(self, action):\n",
    "        \"\"\"Realiza un paso en el juego.\"\"\"\n",
    "        if self.done:\n",
    "            self.message_label.config(text=\"El episodio ya terminó. Presiona 'Reiniciar' para empezar de nuevo.\")\n",
    "            return\n",
    "\n",
    "        # Guardar la última acción para la imagen del agente\n",
    "        self.last_action = action\n",
    "\n",
    "        # Realizar el paso\n",
    "        next_state, reward, terminated = self.lake.step(action)\n",
    "        self.state = next_state\n",
    "        self.done = terminated\n",
    "        self.draw_map()\n",
    "\n",
    "        # Actualizar mensajes\n",
    "        if self.done:\n",
    "            if reward > 0:\n",
    "                self.message_label.config(text=\"¡Ganaste! Llegaste a la meta.\")\n",
    "            else:\n",
    "                self.message_label.config(text=\"¡Perdiste! Caíste en un agujero.\")\n",
    "        else:\n",
    "            self.message_label.config(text=f\"Estado: {self.state}, Acción: {action}, Recompensa: {reward}\")\n",
    "\n",
    "    def reset_game(self):\n",
    "        \"\"\"Reinicia el juego.\"\"\"\n",
    "        self.state = self.lake.reset()\n",
    "        self.done = False\n",
    "        self.last_action = None  # No hay acción previa al reiniciar\n",
    "        self.draw_map()\n",
    "        self.message_label.config(text=\"Presiona un botón para mover al agente.\")\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Crear la ventana de Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"FrozenLake con Imágenes Redimensionadas\")\n",
    "app = FrozenLakeVisualizer(root, lake)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 2: SARSA**\n",
    "- **Objetivo**:\n",
    "  - Implementar el algoritmo SARSA con exploración **ε-greedy**.\n",
    "  - Actualizar una tabla $ Q $ de valores para cada estado y acción.\n",
    "  - Medir el rendimiento contando los episodios exitosos en ventanas de 100 episodios.\n",
    "  - Considerar el problema resuelto si la tasa de éxito supera el 76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "class SARSAVisualizer:\n",
    "    def __init__(self, root, lake, episodes=500):\n",
    "        self.root = root\n",
    "        self.lake = lake\n",
    "        self.episodes = episodes\n",
    "        self.alpha = 0.1  # Tasa de aprendizaje\n",
    "        self.gamma = 0.99  # Factor de descuento\n",
    "        self.epsilon = 0.5  # Probabilidad inicial de exploración\n",
    "        self.q_table = np.zeros((lake.n_rows * lake.n_cols, 4))  # Tabla Q\n",
    "\n",
    "        # Configuración del episodio actual\n",
    "        self.current_episode = 0\n",
    "        self.state = self.lake.reset()\n",
    "        self.action = None\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []  # Historial de éxitos (en ventanas de 100 episodios)\n",
    "\n",
    "        # Configuración de la interfaz gráfica\n",
    "        self.tile_size = 100\n",
    "        self.grid_size = (lake.n_rows, lake.n_cols)\n",
    "        self.load_images()\n",
    "\n",
    "        # Crear el canvas para el mapa\n",
    "        self.canvas = tk.Canvas(\n",
    "            root, width=self.grid_size[1] * self.tile_size, height=self.grid_size[0] * self.tile_size\n",
    "        )\n",
    "        self.canvas.pack()\n",
    "        self.draw_map()\n",
    "\n",
    "        # Botones\n",
    "        self.step_button = tk.Button(root, text=\"Ejecutar SARSA\", command=self.run_sarsa)\n",
    "        self.step_button.pack()\n",
    "\n",
    "        self.reset_button = tk.Button(root, text=\"Reiniciar\", command=self.reset)\n",
    "        self.reset_button.pack(pady=10)\n",
    "\n",
    "        # Mensajes\n",
    "        self.message_label = tk.Label(root, text=\"Presiona 'Ejecutar SARSA' para comenzar.\")\n",
    "        self.message_label.pack()\n",
    "\n",
    "    def load_images(self):\n",
    "        \"\"\"Carga las imágenes necesarias desde la carpeta 'img'.\"\"\"\n",
    "        self.images = {}\n",
    "        img_folder = \"img\"\n",
    "        self.images[\"S\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"F\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"H\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"hole.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"G\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"goal.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"elf_down\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"elf_down.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "\n",
    "    def draw_map(self):\n",
    "        \"\"\"Dibuja el mapa del entorno.\"\"\"\n",
    "        self.canvas.delete(\"all\")  # Limpiar el canvas\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                x, y = j * self.tile_size, i * self.tile_size\n",
    "                tile = self.lake.map_desc[i][j]\n",
    "                self.canvas.create_image(x, y, anchor=tk.NW, image=self.images[tile])\n",
    "\n",
    "        # Dibujar al agente\n",
    "        self.draw_agent()\n",
    "\n",
    "    def draw_agent(self):\n",
    "        \"\"\"Dibuja al agente en la posición actual.\"\"\"\n",
    "        x, y = self.state\n",
    "        x_pixel, y_pixel = y * self.tile_size, x * self.tile_size\n",
    "        self.canvas.create_image(x_pixel, y_pixel, anchor=tk.NW, image=self.images[\"elf_down\"])\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        \"\"\"Selecciona una acción usando ε-greedy.\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice([0, 1, 2, 3])  # Exploración\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Explotación\n",
    "\n",
    "    def run_sarsa(self):\n",
    "        \"\"\"Ejecuta el algoritmo SARSA.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            action = self.epsilon_greedy(state_index)\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                # Tomar un paso en el entorno\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                next_state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "                next_action = self.epsilon_greedy(next_state_index)\n",
    "\n",
    "                # Actualizar la tabla Q usando la regla de SARSA\n",
    "                self.q_table[state_index, action] += self.alpha * (\n",
    "                    reward\n",
    "                    + self.gamma * self.q_table[next_state_index, next_action]\n",
    "                    - self.q_table[state_index, action]\n",
    "                )\n",
    "\n",
    "                # Actualizar estado y acción\n",
    "                state_index, action = next_state_index, next_action\n",
    "                self.state = next_state\n",
    "                done = terminated\n",
    "                step += 1\n",
    "\n",
    "                # Redibujar el mapa\n",
    "                self.draw_map()\n",
    "                self.root.update()\n",
    "\n",
    "                # Verificar si se alcanzó el objetivo\n",
    "                if terminated and reward > 0:\n",
    "                    self.success_count += 1\n",
    "\n",
    "            # Reducir ε después de cada episodio\n",
    "            self.epsilon = max(0.01, self.epsilon * 0.995)\n",
    "\n",
    "            # Actualizar el historial de éxitos cada 100 episodios\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                success_rate = self.success_count / 100\n",
    "                self.success_history.append(success_rate)\n",
    "                self.success_count = 0\n",
    "\n",
    "        # Mostrar resultados finales\n",
    "        self.message_label.config(text=f\"SARSA completado. Tasa de éxito: {self.success_history[-1]:.2f}\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el entorno.\"\"\"\n",
    "        self.state = self.lake.reset()\n",
    "        self.q_table = np.zeros((self.lake.n_rows * self.lake.n_cols, 4))\n",
    "        self.current_episode = 0\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []\n",
    "        self.draw_map()\n",
    "        self.message_label.config(text=\"Presiona 'Ejecutar SARSA' para comenzar.\")\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Crear la ventana de Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"FrozenLake - Ejercicio 2: SARSA\")\n",
    "app = SARSAVisualizer(root, lake, episodes=500)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 3: Q-Learning**\n",
    "- **Objetivo**:\n",
    "  - Sustituir la regla de actualización de SARSA por la de Q-Learning.\n",
    "  - Comparar el rendimiento entre SARSA y Q-Learning para analizar sus diferencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "class QLearningVisualizer:\n",
    "    def __init__(self, root, lake, episodes=500):\n",
    "        self.root = root\n",
    "        self.lake = lake\n",
    "        self.episodes = episodes\n",
    "        self.alpha = 0.5  # Tasa de aprendizaje\n",
    "        self.gamma = 0.99  # Factor de descuento\n",
    "        self.epsilon = 0.5  # Probabilidad inicial de exploración\n",
    "        self.q_table = np.zeros((lake.n_rows * lake.n_cols, 4))  # Tabla Q\n",
    "\n",
    "        # Configuración del episodio actual\n",
    "        self.current_episode = 0\n",
    "        self.state = self.lake.reset()\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []  # Historial de éxitos (en ventanas de 100 episodios)\n",
    "\n",
    "        # Configuración de la interfaz gráfica\n",
    "        self.tile_size = 100\n",
    "        self.grid_size = (lake.n_rows, lake.n_cols)\n",
    "        self.load_images()\n",
    "\n",
    "        # Crear el canvas para el mapa\n",
    "        self.canvas = tk.Canvas(\n",
    "            root, width=self.grid_size[1] * self.tile_size, height=self.grid_size[0] * self.tile_size\n",
    "        )\n",
    "        self.canvas.pack()\n",
    "        self.draw_map()\n",
    "\n",
    "        # Botones\n",
    "        self.step_button = tk.Button(root, text=\"Ejecutar Q-Learning\", command=self.run_q_learning)\n",
    "        self.step_button.pack()\n",
    "\n",
    "        self.reset_button = tk.Button(root, text=\"Reiniciar\", command=self.reset)\n",
    "        self.reset_button.pack(pady=10)\n",
    "\n",
    "        # Mensajes\n",
    "        self.message_label = tk.Label(root, text=\"Presiona 'Ejecutar Q-Learning' para comenzar.\")\n",
    "        self.message_label.pack()\n",
    "\n",
    "    def load_images(self):\n",
    "        \"\"\"Carga las imágenes necesarias desde la carpeta 'img'.\"\"\"\n",
    "        self.images = {}\n",
    "        img_folder = \"img\"\n",
    "        self.images[\"S\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"F\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"H\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"hole.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"G\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"goal.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"elf_down\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"elf_down.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "\n",
    "    def draw_map(self):\n",
    "        \"\"\"Dibuja el mapa del entorno.\"\"\"\n",
    "        self.canvas.delete(\"all\")  # Limpiar el canvas\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                x, y = j * self.tile_size, i * self.tile_size\n",
    "                tile = self.lake.map_desc[i][j]\n",
    "                self.canvas.create_image(x, y, anchor=tk.NW, image=self.images[tile])\n",
    "\n",
    "        # Dibujar al agente\n",
    "        self.draw_agent()\n",
    "\n",
    "    def draw_agent(self):\n",
    "        \"\"\"Dibuja al agente en la posición actual.\"\"\"\n",
    "        x, y = self.state\n",
    "        x_pixel, y_pixel = y * self.tile_size, x * self.tile_size\n",
    "        self.canvas.create_image(x_pixel, y_pixel, anchor=tk.NW, image=self.images[\"elf_down\"])\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        \"\"\"Selecciona una acción usando ε-greedy.\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice([0, 1, 2, 3])  # Exploración\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Explotación\n",
    "\n",
    "    def run_q_learning(self):\n",
    "        \"\"\"Ejecuta el algoritmo Q-Learning.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                # Seleccionar acción usando ε-greedy\n",
    "                action = self.epsilon_greedy(state_index)\n",
    "\n",
    "                # Tomar un paso en el entorno\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                next_state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "\n",
    "                # Actualizar la tabla Q usando la regla de Q-Learning\n",
    "                self.q_table[state_index, action] += self.alpha * (\n",
    "                    reward\n",
    "                    + self.gamma * np.max(self.q_table[next_state_index])\n",
    "                    - self.q_table[state_index, action]\n",
    "                )\n",
    "\n",
    "                # Actualizar estado\n",
    "                state_index = next_state_index\n",
    "                self.state = next_state\n",
    "                done = terminated\n",
    "                step += 1\n",
    "\n",
    "                # Redibujar el mapa\n",
    "                self.draw_map()\n",
    "                self.root.update()\n",
    "\n",
    "                # Verificar si se alcanzó el objetivo\n",
    "                if terminated and reward > 0:\n",
    "                    self.success_count += 1\n",
    "\n",
    "            # Reducir ε después de cada episodio\n",
    "            self.epsilon = max(0.01, self.epsilon * 0.995)\n",
    "\n",
    "            # Actualizar el historial de éxitos cada 100 episodios\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                success_rate = self.success_count / 100\n",
    "                self.success_history.append(success_rate)\n",
    "                self.success_count = 0\n",
    "\n",
    "        # Mostrar resultados finales\n",
    "        self.message_label.config(text=f\"Q-Learning completado. Tasa de éxito: {self.success_history[-1]:.2f}\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el entorno.\"\"\"\n",
    "        self.state = self.lake.reset()\n",
    "        self.q_table = np.zeros((self.lake.n_rows * self.lake.n_cols, 4))\n",
    "        self.current_episode = 0\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []\n",
    "        self.draw_map()\n",
    "        self.message_label.config(text=\"Presiona 'Ejecutar Q-Learning' para comenzar.\")\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Crear la ventana de Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"FrozenLake - Ejercicio 3: Q-Learning\")\n",
    "app = QLearningVisualizer(root, lake, episodes=500)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 4: Exploración Softmax**\n",
    "- **Objetivo**:\n",
    "  - Implementar una estrategia de exploración Softmax.\n",
    "  - Asignar probabilidades de selección de acciones proporcionalmente a sus valores $ Q(s, a) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "class SoftmaxQLearningVisualizer:\n",
    "    def __init__(self, root, lake, episodes=500, tau=1.0):\n",
    "        self.root = root\n",
    "        self.lake = lake\n",
    "        self.episodes = episodes\n",
    "        self.tau = tau  # Temperatura inicial\n",
    "        self.alpha = 0.5  # Tasa de aprendizaje\n",
    "        self.gamma = 0.99  # Factor de descuento\n",
    "        self.q_table = np.zeros((lake.n_rows * lake.n_cols, 4))  # Tabla Q\n",
    "\n",
    "        # Configuración del episodio actual\n",
    "        self.current_episode = 0\n",
    "        self.state = self.lake.reset()\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []  # Historial de éxitos (en ventanas de 100 episodios)\n",
    "\n",
    "        # Configuración de la interfaz gráfica\n",
    "        self.tile_size = 100\n",
    "        self.grid_size = (lake.n_rows, lake.n_cols)\n",
    "        self.load_images()\n",
    "\n",
    "        # Crear el canvas para el mapa\n",
    "        self.canvas = tk.Canvas(\n",
    "            root, width=self.grid_size[1] * self.tile_size, height=self.grid_size[0] * self.tile_size\n",
    "        )\n",
    "        self.canvas.pack()\n",
    "        self.draw_map()\n",
    "\n",
    "        # Botones\n",
    "        self.step_button = tk.Button(root, text=\"Ejecutar Softmax Q-Learning\", command=self.run_softmax_q_learning)\n",
    "        self.step_button.pack()\n",
    "\n",
    "        self.reset_button = tk.Button(root, text=\"Reiniciar\", command=self.reset)\n",
    "        self.reset_button.pack(pady=10)\n",
    "\n",
    "        # Mensajes\n",
    "        self.message_label = tk.Label(root, text=\"Presiona 'Ejecutar Softmax Q-Learning' para comenzar.\")\n",
    "        self.message_label.pack()\n",
    "\n",
    "    def load_images(self):\n",
    "        \"\"\"Carga las imágenes necesarias desde la carpeta 'img'.\"\"\"\n",
    "        self.images = {}\n",
    "        img_folder = \"img\"\n",
    "        self.images[\"S\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"F\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"H\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"hole.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"G\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"goal.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"elf_down\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"elf_down.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "\n",
    "    def draw_map(self):\n",
    "        \"\"\"Dibuja el mapa del entorno.\"\"\"\n",
    "        self.canvas.delete(\"all\")  # Limpiar el canvas\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                x, y = j * self.tile_size, i * self.tile_size\n",
    "                tile = self.lake.map_desc[i][j]\n",
    "                self.canvas.create_image(x, y, anchor=tk.NW, image=self.images[tile])\n",
    "\n",
    "        # Dibujar al agente\n",
    "        self.draw_agent()\n",
    "\n",
    "    def draw_agent(self):\n",
    "        \"\"\"Dibuja al agente en la posición actual.\"\"\"\n",
    "        x, y = self.state\n",
    "        x_pixel, y_pixel = y * self.tile_size, x * self.tile_size\n",
    "        self.canvas.create_image(x_pixel, y_pixel, anchor=tk.NW, image=self.images[\"elf_down\"])\n",
    "\n",
    "    def softmax(self, state):\n",
    "        \"\"\"Calcula las probabilidades de las acciones usando Softmax.\"\"\"\n",
    "        q_values = self.q_table[state]\n",
    "        exp_values = np.exp(q_values / self.tau)\n",
    "        probabilities = exp_values / np.sum(exp_values)\n",
    "        return probabilities\n",
    "\n",
    "    def select_action_softmax(self, state):\n",
    "        \"\"\"Selecciona una acción basada en las probabilidades Softmax.\"\"\"\n",
    "        probabilities = self.softmax(state)\n",
    "        return np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "    def run_softmax_q_learning(self):\n",
    "        \"\"\"Ejecuta el algoritmo Q-Learning con exploración Softmax.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                # Seleccionar acción usando Softmax\n",
    "                action = self.select_action_softmax(state_index)\n",
    "\n",
    "                # Tomar un paso en el entorno\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                next_state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "\n",
    "                # Actualizar la tabla Q usando la regla de Q-Learning\n",
    "                self.q_table[state_index, action] += self.alpha * (\n",
    "                    reward\n",
    "                    + self.gamma * np.max(self.q_table[next_state_index])\n",
    "                    - self.q_table[state_index, action]\n",
    "                )\n",
    "\n",
    "                # Actualizar estado\n",
    "                state_index = next_state_index\n",
    "                self.state = next_state\n",
    "                done = terminated\n",
    "                step += 1\n",
    "\n",
    "                # Redibujar el mapa\n",
    "                self.draw_map()\n",
    "                self.root.update()\n",
    "\n",
    "                # Verificar si se alcanzó el objetivo\n",
    "                if terminated and reward > 0:\n",
    "                    self.success_count += 1\n",
    "\n",
    "            # Reducir temperatura después de cada episodio\n",
    "            self.tau = max(0.1, self.tau * 0.995)\n",
    "\n",
    "            # Actualizar el historial de éxitos cada 100 episodios\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                success_rate = self.success_count / 100\n",
    "                self.success_history.append(success_rate)\n",
    "                self.success_count = 0\n",
    "\n",
    "        # Mostrar resultados finales\n",
    "        self.message_label.config(text=f\"Softmax Q-Learning completado. Tasa de éxito: {self.success_history[-1]:.2f}\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el entorno.\"\"\"\n",
    "        self.state = self.lake.reset()\n",
    "        self.q_table = np.zeros((self.lake.n_rows * self.lake.n_cols, 4))\n",
    "        self.current_episode = 0\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []\n",
    "        self.tau = 1.0  # Restaurar temperatura inicial\n",
    "        self.draw_map()\n",
    "        self.message_label.config(text=\"Presiona 'Ejecutar Softmax Q-Learning' para comenzar.\")\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Crear la ventana de Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"FrozenLake - Ejercicio 4: Softmax Q-Learning\")\n",
    "app = SoftmaxQLearningVisualizer(root, lake, episodes=500, tau=1.0)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 5: Decaimiento de ε**\n",
    "- **Objetivo**:\n",
    "  - Diseñar un esquema para reducir $ \\epsilon $ con el tiempo.\n",
    "  - Evaluar cómo mejora la política aprendida en comparación con la política de exploración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "class DecayEpsilonQLearningVisualizer:\n",
    "    def __init__(self, root, lake, episodes=500, epsilon_0=0.5, epsilon_min=0.01, decay=0.995):\n",
    "        self.root = root\n",
    "        self.lake = lake\n",
    "        self.episodes = episodes\n",
    "        self.epsilon = epsilon_0  # Valor inicial de epsilon\n",
    "        self.epsilon_min = epsilon_min  # Valor mínimo de epsilon\n",
    "        self.decay = decay  # Factor de decaimiento\n",
    "        self.alpha = 0.5  # Tasa de aprendizaje\n",
    "        self.gamma = 0.99  # Factor de descuento\n",
    "        self.q_table = np.zeros((lake.n_rows * lake.n_cols, 4))  # Tabla Q\n",
    "\n",
    "        # Configuración del episodio actual\n",
    "        self.current_episode = 0\n",
    "        self.state = self.lake.reset()\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []  # Historial de éxitos (en ventanas de 100 episodios)\n",
    "\n",
    "        # Configuración de la interfaz gráfica\n",
    "        self.tile_size = 100\n",
    "        self.grid_size = (lake.n_rows, lake.n_cols)\n",
    "        self.load_images()\n",
    "\n",
    "        # Crear el canvas para el mapa\n",
    "        self.canvas = tk.Canvas(\n",
    "            root, width=self.grid_size[1] * self.tile_size, height=self.grid_size[0] * self.tile_size\n",
    "        )\n",
    "        self.canvas.pack()\n",
    "        self.draw_map()\n",
    "\n",
    "        # Botones\n",
    "        self.step_button = tk.Button(root, text=\"Ejecutar Q-Learning con Decaimiento de ε\", command=self.run_q_learning)\n",
    "        self.step_button.pack()\n",
    "\n",
    "        self.reset_button = tk.Button(root, text=\"Reiniciar\", command=self.reset)\n",
    "        self.reset_button.pack(pady=10)\n",
    "\n",
    "        # Mensajes\n",
    "        self.message_label = tk.Label(root, text=\"Presiona 'Ejecutar Q-Learning con Decaimiento de ε' para comenzar.\")\n",
    "        self.message_label.pack()\n",
    "\n",
    "    def load_images(self):\n",
    "        \"\"\"Carga las imágenes necesarias desde la carpeta 'img'.\"\"\"\n",
    "        self.images = {}\n",
    "        img_folder = \"img\"\n",
    "        self.images[\"S\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"F\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"H\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"hole.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"G\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"goal.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"elf_down\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"elf_down.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "\n",
    "    def draw_map(self):\n",
    "        \"\"\"Dibuja el mapa del entorno.\"\"\"\n",
    "        self.canvas.delete(\"all\")  # Limpiar el canvas\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                x, y = j * self.tile_size, i * self.tile_size\n",
    "                tile = self.lake.map_desc[i][j]\n",
    "                self.canvas.create_image(x, y, anchor=tk.NW, image=self.images[tile])\n",
    "\n",
    "        # Dibujar al agente\n",
    "        self.draw_agent()\n",
    "\n",
    "    def draw_agent(self):\n",
    "        \"\"\"Dibuja al agente en la posición actual.\"\"\"\n",
    "        x, y = self.state\n",
    "        x_pixel, y_pixel = y * self.tile_size, x * self.tile_size\n",
    "        self.canvas.create_image(x_pixel, y_pixel, anchor=tk.NW, image=self.images[\"elf_down\"])\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        \"\"\"Selecciona una acción usando ε-greedy.\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice([0, 1, 2, 3])  # Exploración\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Explotación\n",
    "\n",
    "    def run_q_learning(self):\n",
    "        \"\"\"Ejecuta el algoritmo Q-Learning con decaimiento de ε.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                # Seleccionar acción usando ε-greedy\n",
    "                action = self.epsilon_greedy(state_index)\n",
    "\n",
    "                # Tomar un paso en el entorno\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                next_state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "\n",
    "                # Actualizar la tabla Q usando la regla de Q-Learning\n",
    "                self.q_table[state_index, action] += self.alpha * (\n",
    "                    reward\n",
    "                    + self.gamma * np.max(self.q_table[next_state_index])\n",
    "                    - self.q_table[state_index, action]\n",
    "                )\n",
    "\n",
    "                # Actualizar estado\n",
    "                state_index = next_state_index\n",
    "                self.state = next_state\n",
    "                done = terminated\n",
    "                step += 1\n",
    "\n",
    "                # Redibujar el mapa\n",
    "                self.draw_map()\n",
    "                self.root.update()\n",
    "\n",
    "                # Verificar si se alcanzó el objetivo\n",
    "                if terminated and reward > 0:\n",
    "                    self.success_count += 1\n",
    "\n",
    "            # Reducir ε después de cada episodio\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.decay)\n",
    "\n",
    "            # Actualizar el historial de éxitos cada 100 episodios\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                success_rate = self.success_count / 100\n",
    "                self.success_history.append(success_rate)\n",
    "                self.success_count = 0\n",
    "\n",
    "        # Mostrar resultados finales\n",
    "        self.message_label.config(text=f\"Q-Learning completado. Tasa de éxito: {self.success_history[-1]:.2f}\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el entorno.\"\"\"\n",
    "        self.state = self.lake.reset()\n",
    "        self.q_table = np.zeros((self.lake.n_rows * self.lake.n_cols, 4))\n",
    "        self.epsilon = 0.5  # Restaurar epsilon inicial\n",
    "        self.current_episode = 0\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []\n",
    "        self.draw_map()\n",
    "        self.message_label.config(text=\"Presiona 'Ejecutar Q-Learning con Decaimiento de ε' para comenzar.\")\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Crear la ventana de Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"FrozenLake - Ejercicio 5: Q-Learning con Decaimiento de ε\")\n",
    "app = DecayEpsilonQLearningVisualizer(root, lake, episodes=500, epsilon_0=0.5, epsilon_min=0.01, decay=0.995)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 6: Evaluación Apropiada**\n",
    "- **Objetivo**:\n",
    "  - Implementar un bucle interno para evaluar las políticas aprendidas repetidamente.\n",
    "  - Comparar el rendimiento promedio entre SARSA y Q-Learning.\n",
    "  - Concluir en qué circunstancias un algoritmo es mejor que el otro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FrozenLake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 64\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConclusión: Ambas políticas tuvieron un rendimiento similar.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Ejemplo de uso\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Crear el entorno FrozenLake\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m lake \u001b[38;5;241m=\u001b[39m \u001b[43mFrozenLake\u001b[49m(is_slippery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Supongamos que tenemos tablas Q aprendidas para SARSA y Q-Learning\u001b[39;00m\n\u001b[0;32m     67\u001b[0m q_table_sarsa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(lake\u001b[38;5;241m.\u001b[39mn_rows \u001b[38;5;241m*\u001b[39m lake\u001b[38;5;241m.\u001b[39mn_cols, \u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# Simulada para ejemplo\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FrozenLake' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PolicyEvaluator:\n",
    "    def __init__(self, lake, q_table_sarsa, q_table_qlearning, eval_episodes=100):\n",
    "        self.lake = lake\n",
    "        self.q_table_sarsa = q_table_sarsa\n",
    "        self.q_table_qlearning = q_table_qlearning\n",
    "        self.eval_episodes = eval_episodes\n",
    "\n",
    "    def evaluate_policy(self, q_table):\n",
    "        \"\"\"Evalúa una política aprendida representada por una tabla Q.\"\"\"\n",
    "        success_count = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(self.eval_episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # Seleccionar la mejor acción según la tabla Q\n",
    "                action = np.argmax(q_table[state_index])\n",
    "\n",
    "                # Tomar un paso en el entorno\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                next_state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "\n",
    "                # Acumular recompensa\n",
    "                total_reward += reward\n",
    "\n",
    "                # Verificar si el episodio fue exitoso\n",
    "                if terminated and reward > 0:\n",
    "                    success_count += 1\n",
    "\n",
    "                # Actualizar estado\n",
    "                state_index = next_state_index\n",
    "                done = terminated\n",
    "\n",
    "        # Calcular métricas\n",
    "        success_rate = success_count / self.eval_episodes\n",
    "        avg_reward = total_reward / self.eval_episodes\n",
    "        return success_rate, avg_reward\n",
    "\n",
    "    def compare_policies(self):\n",
    "        \"\"\"Compara las políticas aprendidas por SARSA y Q-Learning.\"\"\"\n",
    "        print(\"Evaluando política SARSA...\")\n",
    "        success_rate_sarsa, avg_reward_sarsa = self.evaluate_policy(self.q_table_sarsa)\n",
    "        print(f\"Tasa de éxito SARSA: {success_rate_sarsa:.2f}, Recompensa promedio: {avg_reward_sarsa:.2f}\")\n",
    "\n",
    "        print(\"Evaluando política Q-Learning...\")\n",
    "        success_rate_qlearning, avg_reward_qlearning = self.evaluate_policy(self.q_table_qlearning)\n",
    "        print(f\"Tasa de éxito Q-Learning: {success_rate_qlearning:.2f}, Recompensa promedio: {avg_reward_qlearning:.2f}\")\n",
    "\n",
    "        if success_rate_qlearning > success_rate_sarsa:\n",
    "            print(\"\\nConclusión: Q-Learning superó a SARSA en términos de tasa de éxito.\")\n",
    "        elif success_rate_qlearning < success_rate_sarsa:\n",
    "            print(\"\\nConclusión: SARSA superó a Q-Learning en términos de tasa de éxito.\")\n",
    "        else:\n",
    "            print(\"\\nConclusión: Ambas políticas tuvieron un rendimiento similar.\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Supongamos que tenemos tablas Q aprendidas para SARSA y Q-Learning\n",
    "q_table_sarsa = np.random.rand(lake.n_rows * lake.n_cols, 4)  # Simulada para ejemplo\n",
    "q_table_qlearning = np.random.rand(lake.n_rows * lake.n_cols, 4)  # Simulada para ejemplo\n",
    "\n",
    "# Crear el evaluador\n",
    "evaluator = PolicyEvaluator(lake, q_table_sarsa, q_table_qlearning, eval_episodes=100)\n",
    "\n",
    "# Comparar las políticas\n",
    "evaluator.compare_policies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
