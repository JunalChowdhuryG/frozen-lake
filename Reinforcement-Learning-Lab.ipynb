{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Laboratorio de aprendizaje por refuerzo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo es un enfoque general para aprender una política de control en sistemas dinámicos estocásticos a través de la interacción. En este paradigma, un agente aprende a optimizar su comportamiento acumulando recompensas con el tiempo. A diferencia de la optimización local, el aprendizaje por refuerzo se enfoca en descubrir el control óptimo incluso cuando las recompensas están diferidas en el tiempo.\n",
    "\n",
    "En este laboratorio, exploraremos dos algoritmos clave:\n",
    "1. **SARSA** (*State-Action-Reward-State-Action*): Un algoritmo *on-policy* que actualiza los valores de las acciones en función de la política que sigue el agente.\n",
    "2. **Q-Learning**: Un algoritmo *off-policy* que estima el valor de la política óptima independientemente de la política actual del agente.\n",
    "\n",
    "Además, abordaremos el dilema de **exploración-explotación**, donde el agente debe balancear entre:\n",
    "- **Explorar**: Probar nuevas acciones para descubrir más sobre el entorno.\n",
    "- **Explotar**: Utilizar lo aprendido para maximizar las recompensas conocidas.\n",
    "\n",
    "Para simplificar el aprendizaje de los algoritmos, utilizaremos el entorno **FrozenLake-v0** de OpenAI Gym, un problema sencillo que permite ilustrar los conceptos básicos del aprendizaje por refuerzo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marco Teórico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Elementos del Aprendizaje por Refuerzo**\n",
    "1. **Agente**: Entidad que toma decisiones.\n",
    "2. **Entorno**: Sistema en el que opera el agente.\n",
    "3. **Estados ($ S $)**: Configuraciones posibles del entorno.\n",
    "4. **Acciones ($ A $)**: Conjunto de decisiones posibles para el agente.\n",
    "5. **Recompensas ($ R $)**: Retroalimentación numérica que guía al agente.\n",
    "6. **Política ($ \\pi $)**: Estrategia que sigue el agente para tomar decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Elementos del Aprendizaje por Refuerzo en el Contexto de FrozenLake**\n",
    "\n",
    "#### 1. **Agente**:\n",
    "- **Definición**: El agente es quien toma decisiones en el entorno para maximizar la recompensa acumulada.\n",
    "- **En FrozenLake**: \n",
    "  - El agente es el jugador que debe moverse desde la posición inicial `S` (Start) hasta la meta `G` (Goal) evitando caer en los agujeros `H`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Entorno**:\n",
    "- **Definición**: El sistema en el que opera el agente. Proporciona estados y recompensas en respuesta a las acciones tomadas por el agente.\n",
    "- **En FrozenLake**:\n",
    "  - El entorno es el mapa representado como una cuadrícula:\n",
    "    - `S`: Inicio.\n",
    "    - `F`: Hielo congelado (camino seguro).\n",
    "    - `H`: Agujeros (estado terminal negativo).\n",
    "    - `G`: Meta (estado terminal positivo).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Estados ($ S $)**:\n",
    "- **Definición**: Representan las configuraciones posibles del entorno en un momento dado.\n",
    "- **En FrozenLake**:\n",
    "  - Los estados corresponden a las posiciones del agente en la cuadrícula.\n",
    "  - Por ejemplo, en un mapa de 4x4, hay $ 16 $ estados posibles numerados de $ 0 $ a $ 15 $.\n",
    "  - El estado se calcula como:\n",
    "    \\[\n",
    "    \\text{Estado} = \\text{fila} \\times \\text{número de columnas} + \\text{columna}\n",
    "    \\]\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Acciones ($ A $)**:\n",
    "- **Definición**: Conjunto de decisiones que el agente puede tomar desde un estado dado.\n",
    "- **En FrozenLake**:\n",
    "  - El agente tiene 4 acciones disponibles:\n",
    "    - `0`: Izquierda.\n",
    "    - `1`: Abajo.\n",
    "    - `2`: Derecha.\n",
    "    - `3`: Arriba.\n",
    "  - En modo resbaladizo (`is_slippery=True`), la acción tomada puede no coincidir con el movimiento efectivo debido a la estocasticidad.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Recompensas ($ R $)**:\n",
    "- **Definición**: Retroalimentación numérica que guía al agente hacia su objetivo.\n",
    "- **En FrozenLake**:\n",
    "  - Las recompensas están definidas como:\n",
    "    - **Meta (`G`)**: $ +1 $.\n",
    "    - **Hielo (`F`) o Inicio (`S`)**: $ 0 $.\n",
    "    - **Agujero (`H`)**: $ 0 $.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Política ($ \\pi $)**:\n",
    "- **Definición**: Estrategia que sigue el agente para tomar decisiones en cada estado.\n",
    "- **En FrozenLake**:\n",
    "  - La política puede ser:\n",
    "    - **Inicial**: Selección aleatoria de acciones (exploración).\n",
    "    - **Aprendida**: Después de entrenar al agente, la política selecciona la acción con el mayor valor $ Q(s, a) $ en cada estado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Algoritmos de Aprendizaje**\n",
    "\n",
    "1. **SARSA**:\n",
    "   - SARSA es un algoritmo *on-policy*, lo que significa que actualiza los valores $ Q(s, a) $ basándose en la política actual.\n",
    "   - Ecuación de actualización:\n",
    "     $$\n",
    "     Q_{t+1}(s_t, a_t) \\leftarrow Q_t(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q_t(s_{t+1}, a_{t+1}) - Q_t(s_t, a_t) \\right]\n",
    "     $$\n",
    "     Donde:\n",
    "     - $ \\alpha $: Tasa de aprendizaje.\n",
    "     - $ \\gamma $: Factor de descuento.\n",
    "     - $ r_t $: Recompensa en el paso $ t $.\n",
    "\n",
    "2. **Q-Learning**:\n",
    "   - Q-Learning es un algoritmo *off-policy* que busca la política óptima sin seguir necesariamente la política actual.\n",
    "   - Ecuación de actualización:\n",
    "     $$\n",
    "     Q_{t+1}(s_t, a_t) \\leftarrow Q_t(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_b Q_t(s_{t+1}, b) - Q_t(s_t, a_t) \\right]\n",
    "     $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Estrategias de Exploración**\n",
    "1. **ε-greedy**:\n",
    "   - Con probabilidad $ 1-\\epsilon $, el agente elige la mejor acción conocida.\n",
    "   - Con probabilidad $ \\epsilon $, selecciona una acción aleatoria para explorar.\n",
    "2. **Softmax**:\n",
    "   - Asigna probabilidades a cada acción según sus valores $ Q(s, a) $:\n",
    "     $$\n",
    "     P(a_i|s) = \\frac{e^{\\frac{Q(s, a_i)}{\\tau}}}{\\sum_j e^{\\frac{Q(s, a_j)}{\\tau}}}\n",
    "     $$\n",
    "     Donde $ \\tau $ controla el nivel de exploración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluación y Rendimiento**\n",
    "1. Para evaluar el aprendizaje, es esencial contar los éxitos en ventanas de episodios (por ejemplo, 100).\n",
    "2. Las políticas aprendidas deben evaluarse repetidamente para obtener un promedio confiable, especialmente en entornos estocásticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class FrozenLake:\n",
    "    def __init__(self, map_desc=None, is_slippery=True):\n",
    "        self.map_desc = map_desc or [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ]\n",
    "        self.is_slippery = is_slippery\n",
    "        self.n_rows = len(self.map_desc)\n",
    "        self.n_cols = len(self.map_desc[0])\n",
    "        self.start_state = self._find_start()\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "\n",
    "        # Diccionario de movimientos (acciones) corregido\n",
    "        self.actions = {\n",
    "            0: (0, -1),  # Izquierda\n",
    "            1: (1, 0),   # Abajo\n",
    "            2: (0, 1),   # Derecha\n",
    "            3: (-1, 0)   # Arriba\n",
    "        }\n",
    "\n",
    "        # Diccionario para acciones perpendiculares\n",
    "        self.perpendicular_actions = {\n",
    "            0: [3, 1],  # Izquierda: Arriba y Abajo\n",
    "            1: [0, 2],  # Abajo: Izquierda y Derecha\n",
    "            2: [1, 3],  # Derecha: Abajo y Arriba\n",
    "            3: [0, 2]   # Arriba: Izquierda y Derecha\n",
    "        }\n",
    "\n",
    "    def _find_start(self):\n",
    "        \"\"\"Encuentra la posición inicial (S) en el mapa.\"\"\"\n",
    "        for row in range(self.n_rows):\n",
    "            for col in range(self.n_cols):\n",
    "                if self.map_desc[row][col] == \"S\":\n",
    "                    return (row, col)\n",
    "        raise ValueError(\"Mapa no contiene estado inicial 'S'.\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el juego al estado inicial.\"\"\"\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "        return self.state\n",
    "\n",
    "    def _inside_bounds(self, row, col):\n",
    "        \"\"\"Verifica si la posición está dentro del mapa.\"\"\"\n",
    "        return 0 <= row < self.n_rows and 0 <= col < self.n_cols\n",
    "\n",
    "    def is_hole(self, state):\n",
    "        \"\"\"Verifica si un estado es un agujero.\"\"\"\n",
    "        row, col = state\n",
    "        return self.map_desc[row][col] == \"H\"\n",
    "\n",
    "    def is_goal(self, state):\n",
    "        \"\"\"Verifica si un estado es el objetivo.\"\"\"\n",
    "        row, col = state\n",
    "        return self.map_desc[row][col] == \"G\"\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Aplica una acción y devuelve el nuevo estado, recompensa, y si terminó.\"\"\"\n",
    "        if self.terminated:\n",
    "            raise ValueError(\"El episodio ya terminó. Reinicia el juego con reset().\")\n",
    "\n",
    "        # Aplicar estocasticidad si is_slippery=True\n",
    "        if self.is_slippery:\n",
    "            probabilities = [1/3, 1/3, 1/3]\n",
    "            actions = [action] + self.perpendicular_actions[action]\n",
    "            action = random.choices(actions, probabilities)[0]\n",
    "\n",
    "        # Determinar la nueva posición\n",
    "        move = self.actions[action]\n",
    "        new_row = self.state[0] + move[0]\n",
    "        new_col = self.state[1] + move[1]\n",
    "\n",
    "        # Si la nueva posición está fuera de los límites, el agente no se mueve\n",
    "        if not self._inside_bounds(new_row, new_col):\n",
    "            new_row, new_col = self.state\n",
    "\n",
    "        # Actualizar estado\n",
    "        new_state = (new_row, new_col)\n",
    "        tile = self.map_desc[new_row][new_col]\n",
    "\n",
    "        # Determinar recompensa y si terminó el episodio\n",
    "        reward = -0.1  # Penalización por paso\n",
    "        if tile == \"H\":  # Agujero\n",
    "            reward = -1\n",
    "            self.terminated = True\n",
    "        elif tile == \"G\":  # Meta\n",
    "            reward = 1\n",
    "            self.terminated = True\n",
    "\n",
    "        self.state = new_state\n",
    "        return new_state, reward, self.terminated\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Muestra el mapa actual con la posición del agente.\"\"\"\n",
    "        for row in range(self.n_rows):\n",
    "            row_str = \"\"\n",
    "            for col in range(self.n_cols):\n",
    "                if (row, col) == self.state:\n",
    "                    row_str += \"A\"  # Agente\n",
    "                else:\n",
    "                    row_str += self.map_desc[row][col]\n",
    "            print(row_str)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicios**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 1: Familiarización con OpenAI Gym**\n",
    "Este ejercicio tiene como propósito entender cómo interactuar con el entorno FrozenLake-v1 y explorar sus características principales. Esto incluye:\n",
    "- **Objetivo**:\n",
    "  - Familiarizarse con los espacios de observación y acción.\n",
    "  - Observar cómo las acciones afectan el estado y la recompensa.\n",
    "  - Experimentar con los parámetros clave, como:\n",
    "    - Mapas personalizados.\n",
    "    - Naturaleza resbaladiza del entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class FrozenLake:\n",
    "    def __init__(self, map_desc=None, is_slippery=True):\n",
    "        self.map_desc = map_desc or [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ]\n",
    "        self.is_slippery = is_slippery\n",
    "        self.n_rows = len(self.map_desc)\n",
    "        self.n_cols = len(self.map_desc[0])\n",
    "        self.start_state = self._find_start()\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "\n",
    "        # Diccionario de movimientos (acciones) corregido\n",
    "        self.actions = {\n",
    "            0: (0, -1),  # Izquierda\n",
    "            1: (1, 0),   # Abajo\n",
    "            2: (0, 1),   # Derecha\n",
    "            3: (-1, 0)   # Arriba\n",
    "        }\n",
    "\n",
    "        # Diccionario para acciones perpendiculares\n",
    "        self.perpendicular_actions = {\n",
    "            0: [3, 1],  # Izquierda: Arriba y Abajo\n",
    "            1: [0, 2],  # Abajo: Izquierda y Derecha\n",
    "            2: [1, 3],  # Derecha: Abajo y Arriba\n",
    "            3: [0, 2]   # Arriba: Izquierda y Derecha\n",
    "        }\n",
    "\n",
    "    def _find_start(self):\n",
    "        \"\"\"Encuentra la posición inicial (S) en el mapa.\"\"\"\n",
    "        for row in range(self.n_rows):\n",
    "            for col in range(self.n_cols):\n",
    "                if self.map_desc[row][col] == \"S\":\n",
    "                    return (row, col)\n",
    "        raise ValueError(\"Mapa no contiene estado inicial 'S'.\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el juego al estado inicial.\"\"\"\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "        return self.state\n",
    "\n",
    "    def _inside_bounds(self, row, col):\n",
    "        \"\"\"Verifica si la posición está dentro del mapa.\"\"\"\n",
    "        return 0 <= row < self.n_rows and 0 <= col < self.n_cols\n",
    "\n",
    "    def is_hole(self, state):\n",
    "        \"\"\"Verifica si un estado es un agujero.\"\"\"\n",
    "        row, col = state\n",
    "        return self.map_desc[row][col] == \"H\"\n",
    "\n",
    "    def is_goal(self, state):\n",
    "        \"\"\"Verifica si un estado es el objetivo.\"\"\"\n",
    "        row, col = state\n",
    "        return self.map_desc[row][col] == \"G\"\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Aplica una acción y devuelve el nuevo estado, recompensa, y si terminó.\"\"\"\n",
    "        if self.terminated:\n",
    "            raise ValueError(\"El episodio ya terminó. Reinicia el juego con reset().\")\n",
    "\n",
    "        # Aplicar estocasticidad si is_slippery=True\n",
    "        if self.is_slippery:\n",
    "            probabilities = [1/3, 1/3, 1/3]\n",
    "            actions = [action] + self.perpendicular_actions[action]\n",
    "            action = random.choices(actions, probabilities)[0]\n",
    "\n",
    "        # Determinar la nueva posición\n",
    "        move = self.actions[action]\n",
    "        new_row = self.state[0] + move[0]\n",
    "        new_col = self.state[1] + move[1]\n",
    "\n",
    "        # Si la nueva posición está fuera de los límites, el agente no se mueve\n",
    "        if not self._inside_bounds(new_row, new_col):\n",
    "            new_row, new_col = self.state\n",
    "\n",
    "        # Actualizar estado\n",
    "        new_state = (new_row, new_col)\n",
    "        tile = self.map_desc[new_row][new_col]\n",
    "\n",
    "        # Determinar recompensa y si terminó el episodio\n",
    "        reward = -0.1  # Penalización por paso\n",
    "        if tile == \"H\":  # Agujero\n",
    "            reward = -1\n",
    "            self.terminated = True\n",
    "        elif tile == \"G\":  # Meta\n",
    "            reward = 1\n",
    "            self.terminated = True\n",
    "\n",
    "        self.state = new_state\n",
    "        return new_state, reward, self.terminated\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Muestra el mapa actual con la posición del agente.\"\"\"\n",
    "        for row in range(self.n_rows):\n",
    "            row_str = \"\"\n",
    "            for col in range(self.n_cols):\n",
    "                if (row, col) == self.state:\n",
    "                    row_str += \"A\"  # Agente\n",
    "                else:\n",
    "                    row_str += self.map_desc[row][col]\n",
    "            print(row_str)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 2: SARSA**\n",
    "- **Objetivo**:\n",
    "  - Implementar el algoritmo SARSA con exploración **ε-greedy**.\n",
    "  - Actualizar una tabla $ Q $ de valores para cada estado y acción.\n",
    "  - Medir el rendimiento contando los episodios exitosos en ventanas de 100 episodios.\n",
    "  - Considerar el problema resuelto si la tasa de éxito supera el 76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 100/3000: Tasa de éxito: 2.00%\n",
      "Episodio 200/3000: Tasa de éxito: 7.50%\n",
      "Episodio 300/3000: Tasa de éxito: 9.67%\n",
      "Episodio 400/3000: Tasa de éxito: 15.00%\n",
      "Episodio 500/3000: Tasa de éxito: 20.80%\n",
      "Episodio 600/3000: Tasa de éxito: 24.17%\n",
      "Episodio 700/3000: Tasa de éxito: 29.57%\n",
      "Episodio 800/3000: Tasa de éxito: 33.12%\n",
      "Episodio 900/3000: Tasa de éxito: 36.11%\n",
      "Episodio 1000/3000: Tasa de éxito: 38.00%\n",
      "Episodio 1100/3000: Tasa de éxito: 40.00%\n",
      "Episodio 1200/3000: Tasa de éxito: 40.67%\n",
      "Episodio 1300/3000: Tasa de éxito: 41.31%\n",
      "Episodio 1400/3000: Tasa de éxito: 42.43%\n",
      "Episodio 1500/3000: Tasa de éxito: 42.67%\n",
      "Episodio 1600/3000: Tasa de éxito: 43.44%\n",
      "Episodio 1700/3000: Tasa de éxito: 44.18%\n",
      "Episodio 1800/3000: Tasa de éxito: 44.50%\n",
      "Episodio 1900/3000: Tasa de éxito: 45.84%\n",
      "Episodio 2000/3000: Tasa de éxito: 46.80%\n",
      "Episodio 2100/3000: Tasa de éxito: 47.71%\n",
      "Episodio 2200/3000: Tasa de éxito: 48.50%\n",
      "Episodio 2300/3000: Tasa de éxito: 49.17%\n",
      "Episodio 2400/3000: Tasa de éxito: 49.83%\n",
      "Episodio 2500/3000: Tasa de éxito: 50.20%\n",
      "Episodio 2600/3000: Tasa de éxito: 49.81%\n",
      "Episodio 2700/3000: Tasa de éxito: 50.19%\n",
      "Episodio 2800/3000: Tasa de éxito: 50.75%\n",
      "Episodio 2900/3000: Tasa de éxito: 51.31%\n",
      "Episodio 3000/3000: Tasa de éxito: 51.67%\n",
      "\n",
      "Entrenamiento completado.\n",
      "Tasa de éxito final: 51.67%\n",
      "\n",
      "Evaluación completada. Tasa de éxito: 73.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class FrozenLake:\n",
    "    def __init__(self, map_desc=None, is_slippery=True):\n",
    "        self.map_desc = map_desc or [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ]\n",
    "        self.is_slippery = is_slippery\n",
    "        self.n_rows = len(self.map_desc)\n",
    "        self.n_cols = len(self.map_desc[0])\n",
    "        self.start_state = self._find_start()\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "\n",
    "        self.actions = {\n",
    "            0: (0, -1),  # Izquierda\n",
    "            1: (1, 0),   # Abajo\n",
    "            2: (0, 1),   # Derecha\n",
    "            3: (-1, 0)   # Arriba\n",
    "        }\n",
    "\n",
    "        self.perpendicular_actions = {\n",
    "            0: [3, 1],  # Izquierda: Arriba y Abajo\n",
    "            1: [0, 2],  # Abajo: Izquierda y Derecha\n",
    "            2: [1, 3],  # Derecha: Abajo y Arriba\n",
    "            3: [0, 2]   # Arriba: Izquierda y Derecha\n",
    "        }\n",
    "\n",
    "    def _find_start(self):\n",
    "        for row in range(self.n_rows):\n",
    "            for col in range(self.n_cols):\n",
    "                if self.map_desc[row][col] == \"S\":\n",
    "                    return (row, col)\n",
    "        raise ValueError(\"El mapa no contiene un estado inicial 'S'.\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "        return self.state\n",
    "\n",
    "    def _inside_bounds(self, row, col):\n",
    "        return 0 <= row < self.n_rows and 0 <= col < self.n_cols\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.terminated:\n",
    "            raise ValueError(\"El episodio ya terminó. Reinicia el entorno.\")\n",
    "\n",
    "        if self.is_slippery:\n",
    "            probabilities = [1/3, 1/3, 1/3]\n",
    "            actions = [action] + self.perpendicular_actions[action]\n",
    "            action = random.choices(actions, probabilities)[0]\n",
    "\n",
    "        move = self.actions[action]\n",
    "        new_row = self.state[0] + move[0]\n",
    "        new_col = self.state[1] + move[1]\n",
    "\n",
    "        if not self._inside_bounds(new_row, new_col):\n",
    "            new_row, new_col = self.state\n",
    "\n",
    "        new_state = (new_row, new_col)\n",
    "        tile = self.map_desc[new_row][new_col]\n",
    "\n",
    "        reward = 0\n",
    "        if tile == \"H\":  # Agujero\n",
    "            self.terminated = True\n",
    "        elif tile == \"G\":  # Meta\n",
    "            reward = 1\n",
    "            self.terminated = True\n",
    "\n",
    "        self.state = new_state\n",
    "        return new_state, reward, self.terminated\n",
    "\n",
    "def state_to_index(state, n_cols):\n",
    "    return state[0] * n_cols + state[1]\n",
    "\n",
    "def epsilon_greedy(q_table, state_index, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice([0, 1, 2, 3])  # Exploración\n",
    "    else:\n",
    "        return np.argmax(q_table[state_index])  # Explotación\n",
    "\n",
    "def train_sarsa(lake, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.5, epsilon_decay=0.995):\n",
    "    n_states = lake.n_rows * lake.n_cols\n",
    "    n_actions = 4\n",
    "    q_table = np.zeros((n_states, n_actions))\n",
    "    successes = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = lake.reset()\n",
    "        state_index = state_to_index(state, lake.n_cols)\n",
    "        action = epsilon_greedy(q_table, state_index, epsilon)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done = lake.step(action)\n",
    "            next_state_index = state_to_index(next_state, lake.n_cols)\n",
    "            next_action = epsilon_greedy(q_table, next_state_index, epsilon)\n",
    "\n",
    "            # Actualización SARSA\n",
    "            q_table[state_index, action] += alpha * (\n",
    "                reward + gamma * q_table[next_state_index, next_action] - q_table[state_index, action]\n",
    "            )\n",
    "\n",
    "            state_index = next_state_index\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "\n",
    "        if reward > 0:\n",
    "            successes += 1\n",
    "\n",
    "        epsilon = max(0.01, epsilon * epsilon_decay)  # Decaimiento de ε\n",
    "\n",
    "        # Información de progreso\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episodio {episode + 1}/{episodes}: Tasa de éxito: {successes / (episode + 1):.2%}\")\n",
    "\n",
    "    print(\"\\nEntrenamiento completado.\")\n",
    "    print(f\"Tasa de éxito final: {successes / episodes:.2%}\")\n",
    "    return q_table\n",
    "\n",
    "def evaluate_policy(lake, q_table, episodes=100):\n",
    "    successes = 0\n",
    "    for _ in range(episodes):\n",
    "        state = lake.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_index = state_to_index(state, lake.n_cols)\n",
    "            action = np.argmax(q_table[state_index])  # Política Greedy\n",
    "            state, reward, done = lake.step(action)\n",
    "\n",
    "        if reward > 0:\n",
    "            successes += 1\n",
    "\n",
    "    success_rate = successes / episodes\n",
    "    print(f\"\\nEvaluación completada. Tasa de éxito: {success_rate:.2%}\")\n",
    "    return success_rate\n",
    "\n",
    "# Crear el entorno\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Entrenar SARSA\n",
    "q_table = train_sarsa(lake, episodes=3000)\n",
    "\n",
    "# Evaluar la política aprendida\n",
    "evaluate_policy(lake, q_table, episodes=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusión sobre los Resultados**\n",
    "\n",
    "La evaluación del agente muestra una **tasa de éxito del 73%**, mientras que la **tasa final durante el entrenamiento** fue del 51.67%. Este comportamiento destaca lo siguiente:\n",
    "\n",
    "1. **Desempeño del Agente en Entrenamiento**:\n",
    "   - Durante el entrenamiento, el agente sigue una estrategia $ \\epsilon $-greedy que prioriza la exploración. Esto introduce decisiones aleatorias que reducen la tasa de éxito final del entrenamiento, dejándola en 51.67%.\n",
    "\n",
    "2. **Desempeño del Agente en Evaluación**:\n",
    "   - En la evaluación, el agente deja de explorar ($ \\epsilon = 0 $) y sigue exclusivamente la política aprendida. Esto mejora significativamente su desempeño, alcanzando una tasa de éxito del 73%.\n",
    "\n",
    "3. **Calidad de la Política Aprendida**:\n",
    "   - El agente ha aprendido una política que logra buenos resultados en un entorno desafiante. Sin embargo, la diferencia entre el 51.67% y el 73% indica que aún hay margen para optimizar el proceso de aprendizaje.\n",
    "\n",
    "4. **Dirección para Mejoras**:\n",
    "   - Incrementar el número de episodios de entrenamiento.\n",
    "   - Ajustar los parámetros $ \\alpha $ (tasa de aprendizaje) y $ \\epsilon $ (exploración) para lograr una convergencia más rápida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 3: Q-Learning**\n",
    "- **Objetivo**:\n",
    "  - Sustituir la regla de actualización de SARSA por la de Q-Learning.\n",
    "  - Comparar el rendimiento entre SARSA y Q-Learning para analizar sus diferencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 100/5000: Tasa de éxito: 13.00%\n",
      "Episodio 200/5000: Tasa de éxito: 15.50%\n",
      "Episodio 300/5000: Tasa de éxito: 22.00%\n",
      "Episodio 400/5000: Tasa de éxito: 25.25%\n",
      "Episodio 500/5000: Tasa de éxito: 27.00%\n",
      "Episodio 600/5000: Tasa de éxito: 30.50%\n",
      "Episodio 700/5000: Tasa de éxito: 32.71%\n",
      "Episodio 800/5000: Tasa de éxito: 35.50%\n",
      "Episodio 900/5000: Tasa de éxito: 36.89%\n",
      "Episodio 1000/5000: Tasa de éxito: 36.00%\n",
      "Episodio 1100/5000: Tasa de éxito: 36.27%\n",
      "Episodio 1200/5000: Tasa de éxito: 36.58%\n",
      "Episodio 1300/5000: Tasa de éxito: 38.00%\n",
      "Episodio 1400/5000: Tasa de éxito: 37.64%\n",
      "Episodio 1500/5000: Tasa de éxito: 37.00%\n",
      "Episodio 1600/5000: Tasa de éxito: 37.69%\n",
      "Episodio 1700/5000: Tasa de éxito: 38.06%\n",
      "Episodio 1800/5000: Tasa de éxito: 37.72%\n",
      "Episodio 1900/5000: Tasa de éxito: 37.21%\n",
      "Episodio 2000/5000: Tasa de éxito: 37.60%\n",
      "Episodio 2100/5000: Tasa de éxito: 37.52%\n",
      "Episodio 2200/5000: Tasa de éxito: 37.73%\n",
      "Episodio 2300/5000: Tasa de éxito: 37.96%\n",
      "Episodio 2400/5000: Tasa de éxito: 38.46%\n",
      "Episodio 2500/5000: Tasa de éxito: 38.96%\n",
      "Episodio 2600/5000: Tasa de éxito: 39.27%\n",
      "Episodio 2700/5000: Tasa de éxito: 39.81%\n",
      "Episodio 2800/5000: Tasa de éxito: 40.25%\n",
      "Episodio 2900/5000: Tasa de éxito: 40.03%\n",
      "Episodio 3000/5000: Tasa de éxito: 39.90%\n",
      "Episodio 3100/5000: Tasa de éxito: 40.19%\n",
      "Episodio 3200/5000: Tasa de éxito: 40.12%\n",
      "Episodio 3300/5000: Tasa de éxito: 40.06%\n",
      "Episodio 3400/5000: Tasa de éxito: 40.68%\n",
      "Episodio 3500/5000: Tasa de éxito: 40.71%\n",
      "Episodio 3600/5000: Tasa de éxito: 40.61%\n",
      "Episodio 3700/5000: Tasa de éxito: 41.00%\n",
      "Episodio 3800/5000: Tasa de éxito: 41.18%\n",
      "Episodio 3900/5000: Tasa de éxito: 41.26%\n",
      "Episodio 4000/5000: Tasa de éxito: 40.92%\n",
      "Episodio 4100/5000: Tasa de éxito: 41.00%\n",
      "Episodio 4200/5000: Tasa de éxito: 40.98%\n",
      "Episodio 4300/5000: Tasa de éxito: 40.84%\n",
      "Episodio 4400/5000: Tasa de éxito: 41.00%\n",
      "Episodio 4500/5000: Tasa de éxito: 41.20%\n",
      "Episodio 4600/5000: Tasa de éxito: 41.35%\n",
      "Episodio 4700/5000: Tasa de éxito: 41.32%\n",
      "Episodio 4800/5000: Tasa de éxito: 41.56%\n",
      "Episodio 4900/5000: Tasa de éxito: 41.88%\n",
      "Episodio 5000/5000: Tasa de éxito: 41.60%\n",
      "Entrenamiento completado.\n",
      "Tasa de éxito final: 41.60%\n",
      "Evaluación completada. Tasa de éxito: 79.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79.0"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, lake, episodes=5000, alpha=0.7, gamma=0.9, epsilon=0.1):\n",
    "        self.lake = lake\n",
    "        self.episodes = episodes\n",
    "        self.alpha = alpha  # Tasa de aprendizaje\n",
    "        self.gamma = gamma  # Factor de descuento\n",
    "        self.epsilon = epsilon  # Exploración inicial\n",
    "        self.q_table = np.zeros((lake.n_rows * lake.n_cols, 4))  # Inicializar tabla Q\n",
    "\n",
    "    def epsilon_greedy(self, state_index):\n",
    "        \"\"\"Selecciona una acción usando el método ε-greedy.\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice([0, 1, 2, 3])  # Exploración\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_index])  # Explotación\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Entrena el agente usando Q-Learning.\"\"\"\n",
    "        success_count = 0  # Contador de éxitos\n",
    "        success_history = []  # Historial de tasa de éxito\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # Seleccionar acción usando ε-greedy\n",
    "                action = self.epsilon_greedy(state_index)\n",
    "\n",
    "                # Tomar un paso en el entorno\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                next_state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "\n",
    "                # Actualizar tabla Q\n",
    "                self.q_table[state_index, action] += self.alpha * (\n",
    "                    reward\n",
    "                    + self.gamma * np.max(self.q_table[next_state_index])\n",
    "                    - self.q_table[state_index, action]\n",
    "                )\n",
    "\n",
    "                # Actualizar estado\n",
    "                state_index = next_state_index\n",
    "                done = terminated\n",
    "\n",
    "                # Contar éxito si se alcanza la meta\n",
    "                if reward > 0:\n",
    "                    success_count += 1\n",
    "\n",
    "            # Reducir \\( \\epsilon \\) gradualmente\n",
    "            self.epsilon = max(0.01, self.epsilon * 0.995)\n",
    "\n",
    "            # Guardar la tasa de éxito cada 100 episodios\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                success_rate = (success_count / (episode + 1)) * 100\n",
    "                success_history.append(success_rate)\n",
    "                print(f\"Episodio {episode + 1}/{self.episodes}: Tasa de éxito: {success_rate:.2f}%\")\n",
    "\n",
    "        print(\"Entrenamiento completado.\")\n",
    "        print(f\"Tasa de éxito final: {success_rate:.2f}%\")\n",
    "        return success_history\n",
    "\n",
    "    def evaluate(self, evaluation_episodes=100):\n",
    "        \"\"\"Evalúa el agente después del entrenamiento.\"\"\"\n",
    "        success_count = 0\n",
    "\n",
    "        for _ in range(evaluation_episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # Seguir la mejor acción según la tabla Q\n",
    "                action = np.argmax(self.q_table[state_index])\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "                done = terminated\n",
    "\n",
    "                if reward > 0:\n",
    "                    success_count += 1\n",
    "\n",
    "        success_rate = (success_count / evaluation_episodes) * 100\n",
    "        print(f\"Evaluación completada. Tasa de éxito: {success_rate:.2f}%\")\n",
    "        return success_rate\n",
    "\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "class FrozenLake:\n",
    "    def __init__(self, map_desc=None, is_slippery=True):\n",
    "        self.map_desc = map_desc or [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ]\n",
    "        self.is_slippery = is_slippery\n",
    "        self.n_rows = len(self.map_desc)\n",
    "        self.n_cols = len(self.map_desc[0])\n",
    "        self.start_state = self._find_start()\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "        self.actions = {\n",
    "            0: (0, -1),  # Izquierda\n",
    "            1: (1, 0),   # Abajo\n",
    "            2: (0, 1),   # Derecha\n",
    "            3: (-1, 0)   # Arriba\n",
    "        }\n",
    "        self.perpendicular_actions = {\n",
    "            0: [3, 1],\n",
    "            1: [0, 2],\n",
    "            2: [1, 3],\n",
    "            3: [0, 2]\n",
    "        }\n",
    "\n",
    "    def _find_start(self):\n",
    "        for row in range(self.n_rows):\n",
    "            for col in range(self.n_cols):\n",
    "                if self.map_desc[row][col] == \"S\":\n",
    "                    return (row, col)\n",
    "        raise ValueError(\"Mapa no contiene estado inicial 'S'.\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        self.terminated = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.terminated:\n",
    "            raise ValueError(\"El episodio ya terminó. Reinicia el juego con reset().\")\n",
    "\n",
    "        if self.is_slippery:\n",
    "            probabilities = [1/3, 1/3, 1/3]\n",
    "            actions = [action] + self.perpendicular_actions[action]\n",
    "            action = random.choices(actions, probabilities)[0]\n",
    "\n",
    "        move = self.actions[action]\n",
    "        new_row = self.state[0] + move[0]\n",
    "        new_col = self.state[1] + move[1]\n",
    "\n",
    "        if not (0 <= new_row < self.n_rows and 0 <= new_col < self.n_cols):\n",
    "            new_row, new_col = self.state\n",
    "\n",
    "        self.state = (new_row, new_col)\n",
    "        tile = self.map_desc[new_row][new_col]\n",
    "\n",
    "        if tile == \"H\":\n",
    "            self.terminated = True\n",
    "            return self.state, -1, True\n",
    "        elif tile == \"G\":\n",
    "            self.terminated = True\n",
    "            return self.state, 1, True\n",
    "        else:\n",
    "            return self.state, -0.1, False\n",
    "\n",
    "\n",
    "# Configurar el entorno y entrenar\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "agent = QLearning(lake, episodes=5000)\n",
    "agent.train()\n",
    "agent.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 4: Exploración Softmax**\n",
    "- **Objetivo**:\n",
    "  - Implementar una estrategia de exploración Softmax.\n",
    "  - Asignar probabilidades de selección de acciones proporcionalmente a sus valores $ Q(s, a) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "class SoftmaxQLearningVisualizer:\n",
    "    def __init__(self, root, lake, episodes=500, tau=1.0):\n",
    "        self.root = root\n",
    "        self.lake = lake\n",
    "        self.episodes = episodes\n",
    "        self.tau = tau  # Temperatura inicial\n",
    "        self.alpha = 0.5  # Tasa de aprendizaje\n",
    "        self.gamma = 0.99  # Factor de descuento\n",
    "        self.q_table = np.zeros((lake.n_rows * lake.n_cols, 4))  # Tabla Q\n",
    "\n",
    "        # Configuración del episodio actual\n",
    "        self.current_episode = 0\n",
    "        self.state = self.lake.reset()\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []  # Historial de éxitos (en ventanas de 100 episodios)\n",
    "\n",
    "        # Configuración de la interfaz gráfica\n",
    "        self.tile_size = 100\n",
    "        self.grid_size = (lake.n_rows, lake.n_cols)\n",
    "        self.load_images()\n",
    "\n",
    "        # Crear el canvas para el mapa\n",
    "        self.canvas = tk.Canvas(\n",
    "            root, width=self.grid_size[1] * self.tile_size, height=self.grid_size[0] * self.tile_size\n",
    "        )\n",
    "        self.canvas.pack()\n",
    "        self.draw_map()\n",
    "\n",
    "        # Botones\n",
    "        self.step_button = tk.Button(root, text=\"Ejecutar Softmax Q-Learning\", command=self.run_softmax_q_learning)\n",
    "        self.step_button.pack()\n",
    "\n",
    "        self.reset_button = tk.Button(root, text=\"Reiniciar\", command=self.reset)\n",
    "        self.reset_button.pack(pady=10)\n",
    "\n",
    "        # Mensajes\n",
    "        self.message_label = tk.Label(root, text=\"Presiona 'Ejecutar Softmax Q-Learning' para comenzar.\")\n",
    "        self.message_label.pack()\n",
    "\n",
    "    def load_images(self):\n",
    "        \"\"\"Carga las imágenes necesarias desde la carpeta 'img'.\"\"\"\n",
    "        self.images = {}\n",
    "        img_folder = \"img\"\n",
    "        self.images[\"S\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"F\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"H\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"hole.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"G\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"goal.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"elf_down\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"elf_down.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "\n",
    "    def draw_map(self):\n",
    "        \"\"\"Dibuja el mapa del entorno.\"\"\"\n",
    "        self.canvas.delete(\"all\")  # Limpiar el canvas\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                x, y = j * self.tile_size, i * self.tile_size\n",
    "                tile = self.lake.map_desc[i][j]\n",
    "                self.canvas.create_image(x, y, anchor=tk.NW, image=self.images[tile])\n",
    "\n",
    "        # Dibujar al agente\n",
    "        self.draw_agent()\n",
    "\n",
    "    def draw_agent(self):\n",
    "        \"\"\"Dibuja al agente en la posición actual.\"\"\"\n",
    "        x, y = self.state\n",
    "        x_pixel, y_pixel = y * self.tile_size, x * self.tile_size\n",
    "        self.canvas.create_image(x_pixel, y_pixel, anchor=tk.NW, image=self.images[\"elf_down\"])\n",
    "\n",
    "    def softmax(self, state):\n",
    "        \"\"\"Calcula las probabilidades de las acciones usando Softmax.\"\"\"\n",
    "        q_values = self.q_table[state]\n",
    "        exp_values = np.exp(q_values / self.tau)\n",
    "        probabilities = exp_values / np.sum(exp_values)\n",
    "        return probabilities\n",
    "\n",
    "    def select_action_softmax(self, state):\n",
    "        \"\"\"Selecciona una acción basada en las probabilidades Softmax.\"\"\"\n",
    "        probabilities = self.softmax(state)\n",
    "        return np.random.choice(len(probabilities), p=probabilities)\n",
    "\n",
    "    def run_softmax_q_learning(self):\n",
    "        \"\"\"Ejecuta el algoritmo Q-Learning con exploración Softmax.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                # Seleccionar acción usando Softmax\n",
    "                action = self.select_action_softmax(state_index)\n",
    "\n",
    "                # Tomar un paso en el entorno\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                next_state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "\n",
    "                # Actualizar la tabla Q usando la regla de Q-Learning\n",
    "                self.q_table[state_index, action] += self.alpha * (\n",
    "                    reward\n",
    "                    + self.gamma * np.max(self.q_table[next_state_index])\n",
    "                    - self.q_table[state_index, action]\n",
    "                )\n",
    "\n",
    "                # Actualizar estado\n",
    "                state_index = next_state_index\n",
    "                self.state = next_state\n",
    "                done = terminated\n",
    "                step += 1\n",
    "\n",
    "                # Redibujar el mapa\n",
    "                self.draw_map()\n",
    "                self.root.update()\n",
    "\n",
    "                # Verificar si se alcanzó el objetivo\n",
    "                if terminated and reward > 0:\n",
    "                    self.success_count += 1\n",
    "\n",
    "            # Reducir temperatura después de cada episodio\n",
    "            self.tau = max(0.1, self.tau * 0.995)\n",
    "\n",
    "            # Actualizar el historial de éxitos cada 100 episodios\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                success_rate = self.success_count / 100\n",
    "                self.success_history.append(success_rate)\n",
    "                self.success_count = 0\n",
    "\n",
    "        # Mostrar resultados finales\n",
    "        #self.message_label.config(text=f\"Softmax Q-Learning completado. Tasa de éxito: {self.success_history[-1]:.2f}\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el entorno.\"\"\"\n",
    "        self.state = self.lake.reset()\n",
    "        self.q_table = np.zeros((self.lake.n_rows * self.lake.n_cols, 4))\n",
    "        self.current_episode = 0\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []\n",
    "        self.tau = 1.0  # Restaurar temperatura inicial\n",
    "        self.draw_map()\n",
    "        self.message_label.config(text=\"Presiona 'Ejecutar Softmax Q-Learning' para comenzar.\")\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Crear la ventana de Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"FrozenLake - Ejercicio 4: Softmax Q-Learning\")\n",
    "app = SoftmaxQLearningVisualizer(root, lake, episodes=500, tau=1.0)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 100/5000: Tasa de éxito: 7.00%\n",
      "Episodio 200/5000: Tasa de éxito: 16.00%\n",
      "Episodio 300/5000: Tasa de éxito: 19.67%\n",
      "Episodio 400/5000: Tasa de éxito: 23.50%\n",
      "Episodio 500/5000: Tasa de éxito: 25.60%\n",
      "Episodio 600/5000: Tasa de éxito: 29.00%\n",
      "Episodio 700/5000: Tasa de éxito: 30.29%\n",
      "Episodio 800/5000: Tasa de éxito: 30.38%\n",
      "Episodio 900/5000: Tasa de éxito: 32.56%\n",
      "Episodio 1000/5000: Tasa de éxito: 34.30%\n",
      "Episodio 1100/5000: Tasa de éxito: 34.82%\n",
      "Episodio 1200/5000: Tasa de éxito: 36.33%\n",
      "Episodio 1300/5000: Tasa de éxito: 38.08%\n",
      "Episodio 1400/5000: Tasa de éxito: 39.14%\n",
      "Episodio 1500/5000: Tasa de éxito: 38.93%\n",
      "Episodio 1600/5000: Tasa de éxito: 38.50%\n",
      "Episodio 1700/5000: Tasa de éxito: 38.35%\n",
      "Episodio 1800/5000: Tasa de éxito: 39.56%\n",
      "Episodio 1900/5000: Tasa de éxito: 40.00%\n",
      "Episodio 2000/5000: Tasa de éxito: 40.85%\n",
      "Episodio 2100/5000: Tasa de éxito: 41.00%\n",
      "Episodio 2200/5000: Tasa de éxito: 41.27%\n",
      "Episodio 2300/5000: Tasa de éxito: 41.26%\n",
      "Episodio 2400/5000: Tasa de éxito: 41.38%\n",
      "Episodio 2500/5000: Tasa de éxito: 41.72%\n",
      "Episodio 2600/5000: Tasa de éxito: 42.42%\n",
      "Episodio 2700/5000: Tasa de éxito: 42.48%\n",
      "Episodio 2800/5000: Tasa de éxito: 42.11%\n",
      "Episodio 2900/5000: Tasa de éxito: 41.83%\n",
      "Episodio 3000/5000: Tasa de éxito: 41.93%\n",
      "Episodio 3100/5000: Tasa de éxito: 41.68%\n",
      "Episodio 3200/5000: Tasa de éxito: 42.12%\n",
      "Episodio 3300/5000: Tasa de éxito: 42.52%\n",
      "Episodio 3400/5000: Tasa de éxito: 42.79%\n",
      "Episodio 3500/5000: Tasa de éxito: 42.74%\n",
      "Episodio 3600/5000: Tasa de éxito: 43.33%\n",
      "Episodio 3700/5000: Tasa de éxito: 43.57%\n",
      "Episodio 3800/5000: Tasa de éxito: 43.53%\n",
      "Episodio 3900/5000: Tasa de éxito: 43.59%\n",
      "Episodio 4000/5000: Tasa de éxito: 43.60%\n",
      "Episodio 4100/5000: Tasa de éxito: 44.02%\n",
      "Episodio 4200/5000: Tasa de éxito: 44.12%\n",
      "Episodio 4300/5000: Tasa de éxito: 44.44%\n",
      "Episodio 4400/5000: Tasa de éxito: 44.59%\n",
      "Episodio 4500/5000: Tasa de éxito: 44.51%\n",
      "Episodio 4600/5000: Tasa de éxito: 44.46%\n",
      "Episodio 4700/5000: Tasa de éxito: 44.40%\n",
      "Episodio 4800/5000: Tasa de éxito: 44.19%\n",
      "Episodio 4900/5000: Tasa de éxito: 43.96%\n",
      "Episodio 5000/5000: Tasa de éxito: 43.88%\n",
      "Entrenamiento completado.\n",
      "Tasa de éxito final: 43.88%\n",
      "Evaluación completada en 100 episodios.\n",
      "Tasa de éxito: 87.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(agent, lake, evaluation_episodes=100):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo entrenado siguiendo únicamente la política aprendida.\n",
    "    \n",
    "    Parámetros:\n",
    "    - agent: Modelo entrenado con Q-Learning o SARSA.\n",
    "    - lake: Entorno FrozenLake.\n",
    "    - evaluation_episodes: Número de episodios de evaluación.\n",
    "    \n",
    "    Devuelve:\n",
    "    - Tasa de éxito durante la evaluación.\n",
    "    \"\"\"\n",
    "    success_count = 0\n",
    "\n",
    "    for episode in range(evaluation_episodes):\n",
    "        state = lake.reset()  # Reiniciar el entorno\n",
    "        state_index = state[0] * lake.n_cols + state[1]  # Índice del estado\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Seleccionar la mejor acción según la tabla Q\n",
    "            action = np.argmax(agent.q_table[state_index])\n",
    "\n",
    "            # Realizar la acción en el entorno\n",
    "            next_state, reward, terminated = lake.step(action)\n",
    "            state_index = next_state[0] * lake.n_cols + next_state[1]\n",
    "            done = terminated\n",
    "\n",
    "            # Contar éxito si se alcanza la meta\n",
    "            if reward > 0:\n",
    "                success_count += 1\n",
    "\n",
    "    success_rate = (success_count / evaluation_episodes) * 100\n",
    "    print(f\"Evaluación completada en {evaluation_episodes} episodios.\")\n",
    "    print(f\"Tasa de éxito: {success_rate:.2f}%\")\n",
    "    return success_rate\n",
    "\n",
    "\n",
    "# Configurar el entorno FrozenLake - Estocasticidad \n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Entrenar al agente con Q-Learning\n",
    "agent = QLearning(lake, episodes=5000)\n",
    "agent.train()\n",
    "\n",
    "# Evaluar el modelo entrenado\n",
    "evaluate_model(agent, lake, evaluation_episodes=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 5: Decaimiento de ε**\n",
    "- **Objetivo**:\n",
    "  - Diseñar un esquema para reducir $ \\epsilon $ con el tiempo.\n",
    "  - Evaluar cómo mejora la política aprendida en comparación con la política de exploración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "class DecayEpsilonQLearningVisualizer:\n",
    "    def __init__(self, root, lake, episodes=500, epsilon_0=0.5, epsilon_min=0.01, decay=0.995):\n",
    "        self.root = root\n",
    "        self.lake = lake\n",
    "        self.episodes = episodes\n",
    "        self.epsilon = epsilon_0  # Valor inicial de epsilon\n",
    "        self.epsilon_min = epsilon_min  # Valor mínimo de epsilon\n",
    "        self.decay = decay  # Factor de decaimiento\n",
    "        self.alpha = 0.5  # Tasa de aprendizaje\n",
    "        self.gamma = 0.99  # Factor de descuento\n",
    "        self.q_table = np.zeros((lake.n_rows * lake.n_cols, 4))  # Tabla Q\n",
    "\n",
    "        # Configuración del episodio actual\n",
    "        self.current_episode = 0\n",
    "        self.state = self.lake.reset()\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []  # Historial de éxitos (en ventanas de 100 episodios)\n",
    "\n",
    "        # Configuración de la interfaz gráfica\n",
    "        self.tile_size = 100\n",
    "        self.grid_size = (lake.n_rows, lake.n_cols)\n",
    "        self.load_images()\n",
    "\n",
    "        # Crear el canvas para el mapa\n",
    "        self.canvas = tk.Canvas(\n",
    "            root, width=self.grid_size[1] * self.tile_size, height=self.grid_size[0] * self.tile_size\n",
    "        )\n",
    "        self.canvas.pack()\n",
    "        self.draw_map()\n",
    "\n",
    "        # Botones\n",
    "        self.step_button = tk.Button(root, text=\"Ejecutar Q-Learning con Decaimiento de ε\", command=self.run_q_learning)\n",
    "        self.step_button.pack()\n",
    "\n",
    "        self.reset_button = tk.Button(root, text=\"Reiniciar\", command=self.reset)\n",
    "        self.reset_button.pack(pady=10)\n",
    "\n",
    "        # Mensajes\n",
    "        self.message_label = tk.Label(root, text=\"Presiona 'Ejecutar Q-Learning con Decaimiento de ε' para comenzar.\")\n",
    "        self.message_label.pack()\n",
    "\n",
    "    def load_images(self):\n",
    "        \"\"\"Carga las imágenes necesarias desde la carpeta 'img'.\"\"\"\n",
    "        self.images = {}\n",
    "        img_folder = \"img\"\n",
    "        self.images[\"S\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"F\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"ice.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"H\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"hole.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"G\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"goal.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "        self.images[\"elf_down\"] = ImageTk.PhotoImage(\n",
    "            Image.open(os.path.join(img_folder, \"elf_down.png\")).resize((self.tile_size, self.tile_size))\n",
    "        )\n",
    "\n",
    "    def draw_map(self):\n",
    "        \"\"\"Dibuja el mapa del entorno.\"\"\"\n",
    "        self.canvas.delete(\"all\")  # Limpiar el canvas\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                x, y = j * self.tile_size, i * self.tile_size\n",
    "                tile = self.lake.map_desc[i][j]\n",
    "                self.canvas.create_image(x, y, anchor=tk.NW, image=self.images[tile])\n",
    "\n",
    "        # Dibujar al agente\n",
    "        self.draw_agent()\n",
    "\n",
    "    def draw_agent(self):\n",
    "        \"\"\"Dibuja al agente en la posición actual.\"\"\"\n",
    "        x, y = self.state\n",
    "        x_pixel, y_pixel = y * self.tile_size, x * self.tile_size\n",
    "        self.canvas.create_image(x_pixel, y_pixel, anchor=tk.NW, image=self.images[\"elf_down\"])\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        \"\"\"Selecciona una acción usando ε-greedy.\"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice([0, 1, 2, 3])  # Exploración\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Explotación\n",
    "\n",
    "    def run_q_learning(self):\n",
    "        \"\"\"Ejecuta el algoritmo Q-Learning con decaimiento de ε.\"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            while not done:\n",
    "                # Seleccionar acción usando ε-greedy\n",
    "                action = self.epsilon_greedy(state_index)\n",
    "\n",
    "                # Tomar un paso en el entorno\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                next_state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "\n",
    "                # Actualizar la tabla Q usando la regla de Q-Learning\n",
    "                self.q_table[state_index, action] += self.alpha * (\n",
    "                    reward\n",
    "                    + self.gamma * np.max(self.q_table[next_state_index])\n",
    "                    - self.q_table[state_index, action]\n",
    "                )\n",
    "\n",
    "                # Actualizar estado\n",
    "                state_index = next_state_index\n",
    "                self.state = next_state\n",
    "                done = terminated\n",
    "                step += 1\n",
    "\n",
    "                # Redibujar el mapa\n",
    "                self.draw_map()\n",
    "                self.root.update()\n",
    "\n",
    "                # Verificar si se alcanzó el objetivo\n",
    "                if terminated and reward > 0:\n",
    "                    self.success_count += 1\n",
    "\n",
    "            # Reducir ε después de cada episodio\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.decay)\n",
    "\n",
    "            # Actualizar el historial de éxitos cada 100 episodios\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                success_rate = self.success_count / 100\n",
    "                self.success_history.append(success_rate)\n",
    "                self.success_count = 0\n",
    "\n",
    "        # Mostrar resultados finales\n",
    "        self.message_label.config(text=f\"Q-Learning completado. Tasa de éxito: {self.success_history[-1]:.2f}\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia el entorno.\"\"\"\n",
    "        self.state = self.lake.reset()\n",
    "        self.q_table = np.zeros((self.lake.n_rows * self.lake.n_cols, 4))\n",
    "        self.epsilon = 0.5  # Restaurar epsilon inicial\n",
    "        self.current_episode = 0\n",
    "        self.done = False\n",
    "        self.success_count = 0\n",
    "        self.success_history = []\n",
    "        self.draw_map()\n",
    "        self.message_label.config(text=\"Presiona 'Ejecutar Q-Learning con Decaimiento de ε' para comenzar.\")\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Crear la ventana de Tkinter\n",
    "root = tk.Tk()\n",
    "root.title(\"FrozenLake - Ejercicio 5: Q-Learning con Decaimiento de ε\")\n",
    "app = DecayEpsilonQLearningVisualizer(root, lake, episodes=500, epsilon_0=0.5, epsilon_min=0.01, decay=0.995)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 6: Evaluación Apropiada**\n",
    "- **Objetivo**:\n",
    "  - Implementar un bucle interno para evaluar las políticas aprendidas repetidamente.\n",
    "  - Comparar el rendimiento promedio entre SARSA y Q-Learning.\n",
    "  - Concluir en qué circunstancias un algoritmo es mejor que el otro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando política SARSA...\n",
      "Tasa de éxito SARSA: 0.00, Recompensa promedio: 0.00\n",
      "Evaluando política Q-Learning...\n",
      "Tasa de éxito Q-Learning: 0.01, Recompensa promedio: 0.01\n",
      "\n",
      "Conclusión: Q-Learning superó a SARSA en términos de tasa de éxito.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PolicyEvaluator:\n",
    "    def __init__(self, lake, q_table_sarsa, q_table_qlearning, eval_episodes=100):\n",
    "        self.lake = lake\n",
    "        self.q_table_sarsa = q_table_sarsa\n",
    "        self.q_table_qlearning = q_table_qlearning\n",
    "        self.eval_episodes = eval_episodes\n",
    "\n",
    "    def evaluate_policy(self, q_table):\n",
    "        \"\"\"Evalúa una política aprendida representada por una tabla Q.\"\"\"\n",
    "        success_count = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(self.eval_episodes):\n",
    "            state = self.lake.reset()\n",
    "            state_index = state[0] * self.lake.n_cols + state[1]\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # Seleccionar la mejor acción según la tabla Q\n",
    "                action = np.argmax(q_table[state_index])\n",
    "\n",
    "                # Tomar un paso en el entorno\n",
    "                next_state, reward, terminated = self.lake.step(action)\n",
    "                next_state_index = next_state[0] * self.lake.n_cols + next_state[1]\n",
    "\n",
    "                # Acumular recompensa\n",
    "                total_reward += reward\n",
    "\n",
    "                # Verificar si el episodio fue exitoso\n",
    "                if terminated and reward > 0:\n",
    "                    success_count += 1\n",
    "\n",
    "                # Actualizar estado\n",
    "                state_index = next_state_index\n",
    "                done = terminated\n",
    "\n",
    "        # Calcular métricas\n",
    "        success_rate = success_count / self.eval_episodes\n",
    "        avg_reward = total_reward / self.eval_episodes\n",
    "        return success_rate, avg_reward\n",
    "\n",
    "    def compare_policies(self):\n",
    "        \"\"\"Compara las políticas aprendidas por SARSA y Q-Learning.\"\"\"\n",
    "        print(\"Evaluando política SARSA...\")\n",
    "        success_rate_sarsa, avg_reward_sarsa = self.evaluate_policy(self.q_table_sarsa)\n",
    "        print(f\"Tasa de éxito SARSA: {success_rate_sarsa:.2f}, Recompensa promedio: {avg_reward_sarsa:.2f}\")\n",
    "\n",
    "        print(\"Evaluando política Q-Learning...\")\n",
    "        success_rate_qlearning, avg_reward_qlearning = self.evaluate_policy(self.q_table_qlearning)\n",
    "        print(f\"Tasa de éxito Q-Learning: {success_rate_qlearning:.2f}, Recompensa promedio: {avg_reward_qlearning:.2f}\")\n",
    "\n",
    "        if success_rate_qlearning > success_rate_sarsa:\n",
    "            print(\"\\nConclusión: Q-Learning superó a SARSA en términos de tasa de éxito.\")\n",
    "        elif success_rate_qlearning < success_rate_sarsa:\n",
    "            print(\"\\nConclusión: SARSA superó a Q-Learning en términos de tasa de éxito.\")\n",
    "        else:\n",
    "            print(\"\\nConclusión: Ambas políticas tuvieron un rendimiento similar.\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "\n",
    "# Crear el entorno FrozenLake\n",
    "lake = FrozenLake(is_slippery=True)\n",
    "\n",
    "# Supongamos que tenemos tablas Q aprendidas para SARSA y Q-Learning\n",
    "q_table_sarsa = np.random.rand(lake.n_rows * lake.n_cols, 4)  # Simulada para ejemplo\n",
    "q_table_qlearning = np.random.rand(lake.n_rows * lake.n_cols, 4)  # Simulada para ejemplo\n",
    "\n",
    "# Crear el evaluador\n",
    "evaluator = PolicyEvaluator(lake, q_table_sarsa, q_table_qlearning, eval_episodes=100)\n",
    "\n",
    "# Comparar las políticas\n",
    "evaluator.compare_policies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
